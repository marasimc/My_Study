# Layer Normalization

***reference:  https://zhuanlan.zhihu.com/p/54530247***



Batch Normalization（BN）具有的问题：不适用于RNN等动态网络；batchsize较小的时候效果不好。

Layer Normalization（LN）的提出有效的解决BN的这两个问题。

LN和BN不同点是归一化的维度是互相垂直的，如图1所示。在图1中 N 表示样本轴， C 表示通道轴， F 是每个通道的特征数量。BN如右侧所示，它是取不同样本的同一个通道的特征做归一化；LN则是如左侧所示，它取的是同一个样本的不同通道做归一化。

<img src="assets\6. 模型优化之Layer Normalization\image-20221112185807311.png" alt="image-20221112185807311" style="zoom:80%;" />

## 1. BN的问题

### 1.1 BN与Batch Size

​	如图1右侧部分，BN是按照样本数计算归一化统计量的，当样本数很少时，比如说只有4个。这四个样本的均值和方差便不能反映全局的统计分布息，所以基于少量样本的BN的效果会变得很差。在一些场景中，比如说硬件资源受限，在线学习等场景，BN是非常不适用的。

### 1.2 BN与RNN

​	**RNN可以展开成一个隐藏层共享参数的MLP（Multilayer Perceptron，多层感知机）**，随着时间片的增多，展开后的MLP的层数也在增多，最终层数由输入数据的时间片的数量决定，所以**RNN是一个动态的网络**。

在一个batch中，通常各个样本的长度都是不同的，当统计到比较靠后的时间片时，例如图2中 t>4 时，这时只有一个样本还有数据，基于这个样本的统计信息不能反映全局分布，所以这时BN的效果并不好。

另外如果在测试时我们遇到了长度大于任何一个训练样本的测试样本，我们无法找到保存的归一化统计量，所以BN无法运行。

<img src="assets\6. 模型优化之Layer Normalization\image-20221112190928574.png" alt="image-20221112190928574" style="zoom:67%;" />

## 2. LN介绍

### 2.1 MLP中的LN

通过第一节的分析，我们知道BN的两个缺点的产生原因均是因为计算归一化统计量时计算的样本数太少。LN是一个独立于batch size的算法，所以无论样本数多少都不会影响参与LN计算的数据量，从而解决BN的两个问题。LN的做法如图1左侧所示：**根据样本的特征数做归一化**。

先看MLP中的LN。设 H 是一层中隐层节点的数量， l 是MLP的层数，我们可以计算LN的归一化统计量 μ 和 σ ：

<img src="assets\6. 模型优化之Layer Normalization\image-20221112191625070.png" alt="image-20221112191625070" style="zoom:60%;" />

注意上面统计量的计算是和样本数量没有关系的，它的数量只取决于隐层节点的数量，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。通过 μl 和 σl 可以得到归一化后的值 ：

<img src="assets\6. 模型优化之Layer Normalization\image-20221112191714014.png" alt="image-20221112191714014" style="zoom:67%;" />

<img src="assets\6. 模型优化之Layer Normalization\image-20221112191747030.png" alt="image-20221112191747030" style="zoom:67%;" />

### 2.2 RNN中的LN

<img src="assets\6. 模型优化之Layer Normalization\image-20221112191819161.png" alt="image-20221112191819161" style="zoom:67%;" />

### 2.3 LN与ICS和损失平面平滑

LN能减轻ICS吗？当然可以，至少LN将每个训练样本都归一化到了相同的分布上。而在BN的文章中介绍过几乎所有的归一化方法都能起到平滑损失平面的作用。所以从原理上讲，LN能加速收敛速度的。

## 3. 实验



## 4. 总结

LN是和BN非常近似的一种归一化方法，不同的是BN取的是不同样本的同一个特征，而LN取的是同一个样本的不同特征。在BN和LN都能使用的场景中，BN的效果一般优于LN，原因是基于不同数据，同一特征得到的归一化特征更不容易损失信息。

但是有些场景是不能使用BN的，例如batchsize较小或者在RNN中，这时候可以选择使用LN，LN得到的模型更稳定且起到正则化的作用。RNN能应用到小批量和RNN中是因为LN的归一化统计量的计算是和batchsize没有关系的。