# NAS相关论文研究

reference:  [神经架构搜索研究指南，只看这一篇就够了_AI_Derrick Mwiti_InfoQ精选文章](https://www.infoq.cn/article/JgQPbhS7Irx9dlYMuxTu)

<img src="D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105212842419.png" alt="image-20221105212842419" style="zoom:300%;" />



## 基于强化学习的NAS（2016：[Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578)）

​	本文利用循环神经网络(RNN)生成神经网络的模型描述。为了提高 RNN 在验证集上的精度，作者对 RNN 进行了强化学习训练，该方法在 CIFAR-10 数据集上的错误率为 3.65。

本文提出的神经架构搜索是基于梯度的，主要基于以下考虑：**神经网络的结构和连通性可以用变长串来描述**。① 被称为控制器的神经网络用于生成这样的字符串；② 然后字符串指定的子网络根据真实数据进行训练，并在验证集上得到初始的准确率度量； ③ 然后使用这个准确率数据计算策略梯度，再用后者更新控制器。因此，具有较高准确度的结构可以得到较高的选中概率。

![image-20221105183005581](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105183005581.png)

​	神经架构搜索过程中，控制器用于生成神经网络的结构超参数，下图中控制器用于生成一个卷积神经网络。控制器为每一层预测滤波器高度、滤波器宽度、步长与滤波器个数，然后不断循环；预测由softmax分类器执行，然后作为输入，输入到下一个时间步。一旦控制器完成了生成结构的过程，带有这个结构的神经网络就会建立起来，并用它进行训练与验证。

![image-20221105182955699](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105182955699.png)

​	由于子网络的训练需要花费数小时的时间，为了加快控制器的学习过程，作者采用了分布式训练和异步参数更新的方法。

![image-20221105182943446](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105182943446.png)



## 可伸缩图像识别领域的可转移架构学习（2017：[Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012)）

​	本文中，作者**在一个小数据集上搜索架构上的一个组成模块，然后将该模块再转移到一个大数据集上**，这是因为直接使用大型数据集将非常麻烦和耗时。作者在 CIFAR-10 数据集上寻找最佳卷积层，并将其应用于 ImageNet 数据集。具体做法是将该层的更多副本堆叠在一起来实现的。每一层都有自己的参数用于设计卷积架构。作者将这种体系结构称为 NASNet 架构。

​	另外，论文中引入了**正则化技术——ScheduledDropPath，来改进NASNet模型中的泛化性能**。该方法的错误率为 2.4%。最大的 NASNet 模型平均精度达到 43.1%。

​	本文使用了神经体系结构搜索(NAS)框架,本文的方案中，卷积网络的总体结构是人工预置好的，它们由重复几次的卷积单元组成，每个卷积层具有相同的结构，但权重不同。该网络有两种类型的单元：返回相同维度特征图的卷积单元（Normal Cell），以及返回特征图的卷积单元（Reduction Cell）。后者特征图的高度和宽度在卷积输出时减少了一半。

![image-20221105165729858](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105165729858.png)

​	在本文提出的搜索空间中，每个单元接收两个初始隐藏状态作为输入，这两个初始隐藏状态是前两层或输入图像中的两个单元的输出，在给定这两个初始隐藏状态的情况下，控制器RNN递归地预测卷积单元结构的其余部分。

![image-20221105182842867](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105182842867.png)



## 权重共享的高效NAS（2018：[Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/abs/1802.03268)）

​	本文提出了一种称为高效神经架构搜索（ENAS）的方法，在这种方法中，控制器通过在超计算图中搜索最优子图来发现神经架构，该控制器经过训练，可以选出在验证集上获得最佳准确率的子图。然后训练所选子图对应的模型，使正则交叉熵损失最小化，参数通常在子模型之间共享，以便ENAS能够提供更好的性能。在 CIFAR-10 测试中，ENAS 的错误率为 2.89%，而神经结构搜索(NAS)的错误率为 2.65%。

​	本文强制所有子模型共享权值，以避免从零开始训练每个子模型达到收敛，从而提高了 NAS 的效率。

​	本文用单个有向无环图（DAG）表示 NAS 的搜索空间。通过引入一个具有 N 个节点的 DAG，设计出了递归单元，该单元表示局部计算，图中的边表示 N 个节点之间的信息流。ENAS的控制器是一个RNN，它决定在DAG中每个节点上执行哪些计算以及激活哪些边。控制器网络是一个包含100个隐藏单元的LSTM

![image-20221105192528019](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105192528019.png)

<img src="D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105193610542.png" alt="image-20221105193610542" style="zoom:80%;" />

​	在 ENAS 中，需要学习两组参数：控制器 LSTM 的参数和子模型的共享参数。在训练的第一阶段，对子模型的共享参数进行训练。在第二阶段，对控制器 LSTM 的参数进行训练。这两个阶段在 ENAS 的训练期间交替进行。

<img src="D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105193539808.png" alt="image-20221105193539808" style="zoom:70%;" />



## 高效架构搜索的层次化表示（ICLR 2018：[Hierarchical Representations for Efficient Architecture Search](https://arxiv.org/abs/1711.00436)）

​	该网络中提出的算法在 CIFAR-10 上实现了 3.6%的 top-1 误差，在 ImageNet 上实现了 20.3%的 top-1 误差。作者提出了**一种描述神经网络结构的层次化表示方法，证明了用简单的随机搜索可以得到具有竞争力的图像分类网络结构，并提出了一种可扩展的进化搜索方法变体。**

​	对于平面体系结构表示，他们研究了由单源、单汇聚（single-sink）计算图组成的神经网络体系结构家族，该计算图将源处的输入转换为汇聚处的输出。图中的每个节点都对应一个特征图，每个有向边都和某个操作关联，比如池化操作或者卷积操作。此操作转换输入节点中的特征图，并将其传递给输出节点。

![image-20221105194925667](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105194925667.png)

​	对于层次化结构，将在不同层次上有若干个不同的 motifs。在较高层次的 motifs 构建过程中，较低层次的 motifs 被作为构建模组。

<img src="D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105210435541.png" alt="image-20221105210435541" style="zoom:80%;" />



### 渐进神经架构搜索（ECCV 2018：[Progressive Neural Architecture Search](https://arxiv.org/abs/1712.00559)）

​	该方法**采用基于序列模型的优化策略(SMBO)学习卷积神经网络(CNNs)的结构**。搜索算法的任务是识别一个好的卷积单元，而不是一个完整的CNN。每个单元格包含B个块，每个块是应用于两个输入数据的组合运算符，每个输入都可以在组合之前进行转换（例如，通过卷积进行转换）。然后根据训练集的大小和最终CNN所要求的运行时间决定叠加起来的单元数量。

![image-20221105205533172](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105205533172.png)

​	通过使用步长为 1 或步长为 2 的基本单元叠加预定数量的副本，可以将单元叠加转换为 CNN，如上图所示。然后，在步长为 2 的单元之间的步长为 1 的单元数量，调整为最多可以有 N 个。在网络的顶层引入了平均池化和 softmax 分类层。



### Auto-Keras：高效的神经结构搜索系统（2018：[Auto-Keras: An Efficient Neural Architecture Search System](https://arxiv.org/abs/1806.10282)）

​	本文提出了一个框架，使用贝叶斯优化引导网络形变，以提升 NAS 的效率。基于他们的方法，作者构建了一个名为 Auto-Keras 的开源 AutoML 系统。

​	该方法中，**网络的主要组成模块，是在贝叶斯优化算法的指导下，通过神经结构的形变来寻找搜索空间**。NAS 空间不是欧氏空间，因此作者设计了一个神经网络核函数来解决这一问题。核函数是将一个神经结构变形为另一个神经结构的编辑距离。

![image-20221105210412007](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105210412007.png)

​	利用贝叶斯优化来指导网络形态的第二个挑战是采集函数的优化，这些方法不适用于网络形态的树结构的搜索。通过优化树结构空间的采集函数，解决了这一难题。置信度上界(UCB)被选择作为采集函数。

​	该架构的搜索模块是包含贝叶斯优化器和高斯过程的模块。搜索算法在 CPU 上运行，模型训练器模块在 GPU 上进行计算。该模块在分离的进程中用训练数据训练神经网络，以实现并行化。图模块处理神经网络的计算图，并由搜索模块控制，进行网络形态学操作。模型存储是一个包含经过训练的模型的池子。由于这些模型很大，所以它们存储在存储设备上。

<img src="D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105210820450.png" alt="image-20221105210820450" style="zoom:67%;" />



### 基于贝叶斯优化和最优传输的神经架构搜索（2018：[Neural Architecture Search with Bayesian Optimisation and Optimal Transport](https://arxiv.org/abs/1802.07191)）

​	这篇论文提出了一种基于高斯过程(贝叶斯优化，即 BO)的神经结构搜索框架 NASBOT。这是通过在神经网络架构的空间中开发一个距离度量来实现的，该距离度量可以通过最优传输程序来计算。作者提出了一种神经网络结构的(伪)距离，称为 OTMANN(神经网络结构的最优传输度量)，可以通过最优传输程序进行快速计算。他们还开发了一个 BO 框架来优化神经网络结构上的函数，称为 NASBOT(使用贝叶斯优化和最优传输的神经架构搜索)。

​	为了实现 BO 方案，本文提出了一种神经网络结构的核函数，并给出了一种优化神经网络结构采集函数的方法，它采用进化算法对采集函数进行优化。

​	这个方法有一个初始的网络池，并计算这些网络上采集函数。然后该网络池的一组 Nmut 个突变被生成出来。首先要做的就是从被评估的网络集合中随机选择 Nmut 个候选对象，这样那些具有较高的函数值的网络更有可能被选中。然后对每个候选对象进行修改，以生成一个新的架构。

​	可以通过增加或减少一个层中的计算单元数量、添加或删除层，或更改现有层的连接结构来更改架构。

​	最后一步是评估这些 Nmut 突变的采集函数，将其添加到初始池中，并重复指定的次数。在实验中，作者使用 NASBOT 来优化获取。通过实验，他们得出结论，NASBOT 的性能优于用于优化采集的进化算法。

![image-20221105210956041](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105210956041.png)



### SNAS：随机神经结构搜索（ICLR 2019：[SNAS: Stochastic Neural Architecture Search](https://arxiv.org/abs/1812.09926)）

​	这篇论文的作者提出了随机神经结构搜索(SNAS)。SNAS 是 NAS 的端到端解决方案，在同一轮的反向传播中同时训练神经算子参数和体系结构分布参数。在此过程中，它维护了 NAS 流程的完整性和可微性。

​	作者将 NAS 重新表述为单元中搜索空间的联合分布参数的优化问题。搜索梯度被用于，利用梯度信息进行泛化的可微损失的体系结构搜索。这种搜索梯度与基于增强学习的 NAS 优化了相同的目标，但为结构决策分配分数时效率更高。

如下所示，搜索空间使用一个有向无环图(DAG)表示，称为父图。在图中，节点 xi 代表隐式表示。边(i, j)表示要在节点之间选择的信息流和操作。

![image-20221105211411688](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105211411688.png)



### DARTS：可微结构搜索（ICLR 2019：[DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)）

​	这篇论文以可微的方式构造任务，解决了结构搜索的可伸缩性问题。

​	本文没有在一组离散的候选结构上进行搜索，而是将搜索空间放宽为连续的。因此，可以通过梯度下降，对体系结构的验证集性能进行优化。基于梯度优化的数据效率提升，使得 DARTS 能够使用更少的计算资源获得出色的性能。该模型的性能也优于 ENAS。DARTS 既适用于卷积网络，也适用于递归网络。

​	作者寻找一种计算单元作为最终架构的构建模块。通过递归连接，学习单元可以被堆叠成卷积网络或递归网络。一个单元是由 N 个节点的有序序列组成的有向无环图。每个节点都是一个隐式的表示——例如一个特征图——并且每条有向边都与转换节点的一些操作相关联。一个单元格被假定为有两个输入节点和一个输出节点。卷积单元的输入节点定义为前两层的单元输出。最近文献里提到的卷积单元里，它们的输入节点被定义为当前步骤的输入和上一步所携带的状态。对所有中间节点应用缩减操作（例如 concatenation 操作）来生成输出单元。

![image-20221105212127184](D:\文档\项目\My_Study\NAS\assets\1. NAS论文精炼\image-20221105212127184.png)