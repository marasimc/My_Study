# NAS详解

***reference: [NAS详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/52471966)***



​	这里介绍使用强化学习学习一个CNN（NAS-CNN）或者一个RNN cell（NAS-RNN），并且通过最大化网络在验证集上的精度期望来优化网络，在CIFAR-10数据集上，NAS-CNN的错误率已经逼近当时最好的[DenseNet](https://zhuanlan.zhihu.com/p/42708327)[2]，在TreeBank数据集上，NAS-RNN要优于LSTM。

## 1. 背景

​	算法的主要目的是使用强化学习寻找最优网络，包括一个图像分类网络的卷积部分（表示层）和RNN的一个类似于LSTM的cell.

​	由于现在的神经网络一般采用堆叠block的方式搭建而成，这种堆叠的超参数可以通过一个序列来表示。而这种序列的表示方式正是RNN所擅长的工作。所以，NAS会使用一个RNN构成的控制器（controller）以概率 p 随机采样一个网络结构 A ，接着在CIFAR-10上训练这个网络并得到其在验证集上的精度 R ，然后在使用 R 更新控制器的参数，如此循环执行直到模型收敛，如图1所示。

<img src="assets\2. NAS详解\image-20221109213743548.png" alt="image-20221109213743548" style="zoom:80%;" />

## 2. NAS详细介绍

### 2.1 NAS-CNN

​	首先我们考虑最简单的CNN，即只有卷积层构成。那么这种类型的网络是很容易用控制器来表示的。即将控制器分成 N 段，每一段有若干个输出，每个输出表示CNN的一个超参数，例如Filter的高，Filter的宽，横向步长，纵向步长以及Filter的数量，如图2所示。

<img src="assets\2. NAS详解\image-20221109214205561.png" alt="image-20221109214205561" style="zoom:80%;" />

了解了控制器的结构以及控制器如何生成一个卷积网络，唯一剩下的也是最终要的便是如何更新控制器的参数 θc 。

控制器每生成一个网络可以看做一个action，记做 a1:T ，其中 T 是要预测的超参数的数量。当模型收敛时其在验证集上的精度是 R 。我们使用 R 来作为强化学习的奖励信号，也就是说通过调整参数 θc 来最大化 R 的期望，表示为：

<img src="assets\2. NAS详解\image-20221109215611737.png" alt="image-20221109215611737" style="zoom:67%;" />

由于 R 是不可导的，所以我们需要一种可以更新 θc 的策略，NAS中采用的是Williams等人提出的REINFORCE rule：

<img src="assets\2. NAS详解\image-20221109215720794.png" alt="image-20221109215720794" style="zoom:67%;" />

​	上式是梯度的无偏估计，但是往往方差比较大，为了减小方差，算法中使用的是下面的更新值：

<img src="assets\2. NAS详解\image-20221110200838525.png" alt="image-20221110200838525" style="zoom:67%;" />

​	上面得到的控制器的搜索空间是不包含跳跃连接（skip connection）的，所以不能产生类似于ResNet或Inception之类的网络。NAS-CNN是通过在上面的控制器中添加注意力机制来跳跃连接的。

<img src="assets\2. NAS详解\image-20221110201423781.png" alt="image-20221110201423781" style="zoom:90%;" />

​	在第 N 层，我们添加 N−1 个anchor来确定是否需要在该层和之前的某一层添加跳跃连接，这个anchor是通过两层的隐节点状态和sigmoid激活函数来完成判断的，具体的讲：

<img src="assets\2. NAS详解\image-20221110205514646.png" alt="image-20221110205514646" style="zoom:67%;" />

​	由于添加了跳跃连接，而由训练得到的参数可能会产生许多问题，例如某个层和其它所有层都没有产生连接等等，所以有几个问题我们需要注意：

1. 如果一个层和其之前的所有层都没有跳跃连接，那么这层将作为输入层；
2. 如果一个层和其之后的所有层都没有跳跃连接，那么这层将作为输出层，并和所有输出层拼接之后作为分类器的输入；
3. 如果输入层拼接了多个尺寸的输入，则通过将小尺寸输入加值为0的padding的方式进行尺寸统一。

除了卷积和跳跃连接，例如池化，BN，Dropout等策略也可以通过相同的方式添加到控制器中，只不过这时候需要引入更多的策略相关参数了。



### 2.2 NAS-RNN

​	这里主要介绍如何使用一个RNN控制器来描述一个RNN cell。

<img src="assets\2. NAS详解\image-20221111005659109.png" alt="image-20221111005659109" style="zoom:67%;" />

如图6所示，在这个树结构中有两个叶子节点和一个中间节点，这种两个叶子节点的情况简称为base2，而图4的LSTM则是base4。叶子节点的索引是0，1，中间节点的索引是2，如图6左侧部分。也就是说控制器需要预测3个block，每个block包含一个操作（加，点乘等）和一个激活函数（*ReLU*，*sigmoid*，*tanh*等）。在3个block之后接的是一个Cell inject，用于控制 ct−1 的使用，最后是一个Cell indices，确定哪些树用于计算 ct 。

<img src=".\assets\2. NAS详解\image-20221111005803741.png">