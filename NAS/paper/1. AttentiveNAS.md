# AttentiveNAS - Attentive Sampling策略

## 1. 概述

```
提出了Attentive Sampling采样策略，打破了传统的值关注最优的帕累托的思维，关注最优和最差的帕累托两个方面，并取得了当时NAS领域SOTA的效果。

近年来出现了two-stage的NAS，其将训练过程和搜索过程解耦，在准确率和最终效果都很不错，但是在训练过程中需要对搜索空间进行采样，这直接影响了最终模型的准确率，具体来说，由于大部分抽样都采用简单的均匀采样策略，不能保证模型性能的帕累托前沿，可能会错失提高模型精度的机会。另外连续可微的NAS对超参数的选择非常敏感（包括随机种子和数据划分的设置），往往需要多次实验才能获得较好的性能。权值共享的网络，直接继承来的权值往往不是最优解，需要从头开始训练，所以会引入额外的计算开销。

为了解决上述问题，本文提出了AttentiveNAS，改进抽样策略，实现更好的帕累托性能。论文还设计了可以高效识别网络帕累托前沿的算法。不用额外对搜索得到的模型进行训练或者后处理也能得到SOTA效果，在ImageNet取得了80.1%的准确率和 491 MFLOPs的结果。
```

## 2. 简介

```
当前NAS最大的挑战来源于两个方面：一是网络规模非常大，二是计算的代价太大。

NAS一般将参数训练和网络架构优化分为两个独立阶段：
①第一阶段通过权值共享对搜索空间中所有可选网络的参数优化，在训练结束时，所有网络同时达到较优的性能。
②第二阶段利用进化算法等搜索算法，在各种资源约束下找到最优的模型。
两阶段的NAS很大程度上依赖于第一阶段的候选网络训练。为了让所有候选网络的性能效果更优，在训练过程中从搜索空间中对候选网络进行采样，然后通过SGD对每个样本进行优化。


现有的采样方法主要是统一采样策略，但是这种策略使训练阶段和搜索阶段不相关。具体来说，搜索阶段会侧重于准确性和推断效率，而训练阶段并不针对二者进行改进，而将每个候选网络视为同等重要。因此网络失去了进一步提升准确率和效率的空间。因此，本文提出了attentiveNAS，回答了两个问题：
1.训练过程应该采样哪些候选网络？
2.应该如何对候选网络高效采样，才不会在训练中引入太多计算开销？

针对问题1，采用BestUp + WorstUp策略，BestUp用于改进当前最好的帕累托前沿，WorstUp用于提升当前表现最差的候选网络。换句话说，一个提高上限，一个提高下限。通过逼近最坏帕累托集合的极限，可以更新权重共享网络中优化最少的参数，使所有参数得到充分训练。
针对问题2，使用训练损失+用预训练的预测器预测的准确率，来衡量最终模型的准确率。
```

```
本文贡献：
1. 提出了一种新的策略，AttentiveNAS，通过对帕累托最佳（Pareto-best）或最差（Pareto-worst）前沿的网络进行注意抽样来改进现有的两阶段NAS。比较了 BestUp 和 WorstUp 两种不同的采样策略。
2. 提出了两种方法来有效采样最佳或最差的帕累托前沿。
3. 考虑到搜索 AttentiveNAS 模型家族的 FLOPs 限制，本文实现了 SOTA 的 ImageNet 精度。例如，AttentiveNASA0 比 MobileNetV3 的准确率高2.1%，而相比 FBNetV3, AttentiveNAS-A2 的准确率高 0.8%。
```

```

```

