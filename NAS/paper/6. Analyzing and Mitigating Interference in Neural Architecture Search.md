# Analyzing and Mitigating Interference in Neural Architecture Search

> 论文地址：https://proceedings.mlr.press/v162/xu22h.html

## 摘要

权重共享是一种流行的方法，通过重用来自先前训练的子模型的共享算子的权重来降低神经架构搜索 (NAS) 的训练成本。然而，==由于权重共享引起的不同子模型之间的干扰，采样得到的子模型的估计精度和真实精度之间的秩相关性较低==。论文==通过对不同子模型进行采样并计算共享算子的梯度相似度来研究干扰问题==，并观察到：*1）两个子模型之间共享算子的干扰与它们之间不同算子的数量正相关; 2）共享算子的输入输出越相似，干扰越小*。

受这两个观察的启发，提出了两种减轻干扰的方法：

1）MAGIC-T：提出了一种逐步修改方案，通过修改相邻优化步骤之间的一个算子来最小化对共享算子的干扰，而不是随机采样子模型进行优化；

2）MAGIC-A：强制算子在所有子模型中的输入和输出相似，以减少干扰。

在 BERT 搜索空间上进行的实验表明，两种抑制干扰的方法都可以提高 super-net 秩相关性，并且结合两种方法可以获得更好的结果。搜索得到的架构优于 RoBERT~base~ 和ELECTRA~base~。在 BERT 压缩、阅读理解和大规模图像分类任务上的大量结果也证明了论文提出的方法的有效性和通用性。

## 1. Introduction

​		NAS的目标是自动化搜索得到高性能架构的过程，权重共享是在一个包含搜索空间中所有架构的超网上维持一个权重的单一副本，权重共享方法不是对每个架构从头开始进行独立训练，而是在每个训练步骤中从超网采样子模型或子图，并从先前训练过的超网架构中继续对共享算子的权重进行训练。

​		虽然已经提出了许多基于权重共享的NAS方法并取得了令人印象深刻的结果，但许多工作发现==由于不同子模型之间对共享权重的干扰，子模型的估计精度与真实精度之间的秩相关性较低==。共享算子即使使用同一批训练数据，也会从具有不同架构拓扑的子模型接收不同的梯度方向进行优化，这严重影响了子模型的秩，因为干扰会导致训练不足和性能评估不准确。对于包含不同类型候选算子（例如卷积核多头注意力）和众多子模型的复杂搜索空间，这种现象较为严重。

​		之前的研究工作已经注意到干扰问题，并从经验上发现干扰与搜索空间大小等因素相关，但关于干扰的原因以及如何缓解干扰的讨论很少。本文==通过定量分析发现共享算子上的干扰与两个子模型之间存在的不同算子的数量正相关；产生干扰的主要原因是架构的拓扑结构随着训练过程的随机变化，那么不同架构拓扑共享的算子可能在前馈过程中接收来自不同输入的激活，在反向传播过程中接收来自不同输出的梯度，这样共享算子就会被不同的子模型朝着不同的方向进行优化，造成了干扰==。因此，强制算子的输入和输出与所有子模型的平均输入和输出相似，可以减少干扰。

​		根据上述观察结果，本文提出了两种从不同角度减少干扰（mitigating interference, MAGIC）的方法：

1）MAGIC-T：减少相邻采样步间结构拓扑的变化；

首先分析了常用的随机单路径one-shot算法，发现相邻采样步间不同算子的个数与超网的层数、候选算子的个数成正相关，在较大的搜索空间中会造成严重的干扰；为了最大限度地减少干扰，通过采样一个子模型来逐渐改变共享算子的拓扑环境，该子模型不同于上一步采样的子模型，每个训练步骤中只有一个算子。

2）MAGIC-A：对齐不同子模型共享算子的输入和输出；

选择子模型中验证精度最好的模型作为基模型，分别将所有子模型的输入和输出对齐在一起，当任意一个子模型的性能超过基模型时将替换成为基模型。



​		为了验证方法的有效性，采用了一个具有挑战性的混合超网，包括多头注意力、前馈网络和卷积算子，并在大规模BERT预训练任务上进行了实验。实验结果表明，MAGIC-T和MAGIC-A分别能够减轻干扰和提高秩相关性，将它们组合在一起可以获得更好的性能。



本文贡献总结如下：

> - 对NAS中的干扰问题进行了深入分析，发现两个模型之间的干扰是由不同的梯度方向引起的，并且与它们的架构拓扑以及共享算子的输入和输出的差异成正相关。本文是第一个量化地用实验结果来分析干扰的；
> - 提出了两种减轻干扰的方法：MAGIC-T与MAGIC-A，通过减少采样过程中拓扑变化和在不同子模型之间对齐共享算子的输入和输出来减少干扰；
> - 在BERT搜索空间上的实验表明，本文方法可以通过减少干扰来显著提高超网的秩相关性。

## 2. Related Work

- NAS：很多NAS方法采用权重共享的思想，在一个超网中重用先前训练的架构的权重，以加快训练过程。
- 权重共享和干扰问题：先前有论文利用64种架构的搜索空间进行了权重共享干扰的实验研究，通过每一步随机采样架构进行训练并绘制先前采样架构的精度，发现当前使用采样的子模型进行更新对其他模型是不利的，从而导致秩的高方差。相比之下，本文的定量实验分析揭示了干扰的本质是由不同的子模型拓扑结构引起的共享权重上的梯度方向不同。
- 干扰问题的解决：①Zhang at al.提出了根据子模型的相似性来划分搜索空间（例如按字典顺序排序模型，将这些模型平均地分为几组，然后再每个组的模型上训练超网），然而，对于一个大的搜索空间，识别相似的架构是困难的，而微调每个子模型会导致巨大的计算开销。②通过去除训练过程中表现不好的架构和算子来减少干扰，逐步缩小搜索空间；③有论文提出了操作剪枝、逐步搜索空间剪枝、批归一化中去除仿射操作等方法，以降低共享程度。④本文工作通过修改采样过程和对齐共享算子的输入输出来减轻干扰，本文工作旨在定量分析干扰并提出有效的方法来缓解这种干扰。

## 3. Analyzing Interference（分析干扰）

在单路径one-shot的NAS方法中，不同子模型共享的算子接收到不同的梯度导致不同的优化方向，进而导致不同子模型之间的干扰，这种干扰导致训练不足和性能评估不准确，从而影响子模型的排名。如果能够==了解哪些因素可能影响来自不同子模型的梯度差异==，就可以设计更好的解决方案来减轻干扰。

### 3.1 Analysis Setup

​		梯度受到多种因素的影响，包括输入数据、超网权重和选定的子模型。为了研究完全由不同子模型引起的梯度变化，首先使用单路径one-shot NAS算法训练一个超网，然后固定超网权重，只研究同一批训练数据下不同子模型引起的梯度。超网以链式组织，有N层，每一层包含O中的所有候选操作O={o~1~, …, o~C~}，其中C是预定义候选操作的个数，子模型是从底层到顶层的单一路径，因此超网中有C^N^中可能的子模型。

​		为了研究更具有挑战性的环境中的干扰，采用了混合BERT搜索空间，包括多头注意力（multi-head attention, MHA）。前馈网络（FFN）和卷积（conv），并在BERT预训练任务上进行了实验。具体地，为了研究同一类型算子内部和不同类型算子之间的干扰，使用候选算子O={MHA6, MHA8, FFN, FFN', CONV3, CONV5}，其中MHA6是带有6头的MHA；CONV3是核大小为3的卷积；FFN' 是内部hidden size更大的FFN。为了排除不同算子参数大小的影响，进一步调整了算子内部hidden size，以确保它们具有相似的参数大小。在32个NVIDIA P40 gpu上，使用1024个句子的批大小训练一个N=12层的超网，训练步数为62500步；然后固定住超网，通过输入同一批数据（批大小为2048个句子）来研究不同子模型的梯度。

### 3.2 Results and Analyses

（1）首先关注一个简单的情况：子模型中只有一个操作不同。

<img src="assets/6. Analyzing and Mitigating Interference in Neural Architecture Search/image-20230209213743983.png" alt="image-20230209213743983" style="zoom:80%;" />

如Figure 1所示，选择C个子模型，它们只在第2层不同，然后比较它们第一层在O~g~的梯度，为了衡量不同子模型间O~g~的梯度相似性，将梯度拉平并计算它们的余弦相似度，余弦相似度越低表示梯度差异越大，因此对共享算子的干扰越大。

<img src="assets/6. Analyzing and Mitigating Interference in Neural Architecture Search/image-20230209214600496.png" alt="image-20230209214600496" style="zoom:150%;" />

如Figure 2(a)所示，可以发现：①虽然不同的子模型只相差一个算子，但梯度余弦相似度仍然不高，说明不同的子模型实际上可以导致其共享算子上有非常不同的梯度方向；②同类型算子比不同类型算子的梯度干扰更少（余弦相似度更大），说明混合搜索空间中的干扰往往比具有同类型算子的干扰更严重。

对于混合搜索空间，虽然所有的子模型都是在相同的任务下训练的，但并不能保证操作在不同的子模型上学习相同的变换，即：当操作定位在不同的子模型上时，其输入和输出是不同的。因此，如果显式地强制操作符的输入和输出相似，可以减少干扰。为此，重新训练了一个超网，在每一个训练步骤中，首先随机选择C个子模型，并计算它们在每一层中的平均输入和输出，然后随机抽取一个子模型进行训练，并通过逐层计算其输入输出与平均值之间的MSE损失增加一个额外的对齐训练目标，最后固定超网并重新计算梯度余弦相似度，如Figure 2(b)所示，可以观察到，通过对齐共享操作的输入和输出到与平均输入和输出相似，可以减少梯度干扰。

（2）进一步将分析扩展到一个更普遍的情况：子模型间有m个操作不同。特别地，选择C个子模型，从第二层到第(m+1)层不同，并计算操作O~g~的余弦相似度矩阵，然后计算余弦相似度矩阵的平均值以表示m个操作的平均干扰，结果如Figure 2(c)所示，可以看到两个子模型之间对一个共享算子的干扰和它们之间不同算子的数量呈正相关，这证明了在复杂搜索空间的训练过程中随机采样子模型可能会造成严重的干扰，因为它们的架构拓扑可能会有很大的不同。



尽管这些分析是在固定超网并输入同一批数据的条件下进行的，但关于哪些因素影响干扰的发现是一般性的，并且可以帮助开发新的方法来减轻训练期间的干扰。

## 4. Mitigating Interference（减少干扰）

提出了两种方法：MAGIC-T和MAGIC-A来减少干扰。

- MAGIC-T：根据3.2节分析，两个子模型之间的拓扑差异越大，对共享算子的干扰越大。给定一个具有N层和C个候选算子的超网，先前的单路径NAS方法在α~t~和α~t-1~之间平均改变N(C-1)/C个操作，其中α~t~是在第t步采样的子模型（因为有N层，每个操作在α~t-1~到α~t~发生改变的概率为C-1/C），在巨大的搜索空间下，相邻采样步骤之间采样的两个子模型差异很大，共享算子的梯度相互干扰，不利于在训练中取得进展。

  为了减轻这种干扰，提出了MAGIC-T来逐步改变拓扑环境。具体地，在每个训练步骤中，MAGIC-T都会通过随机将子模型α~t-1~中的一个操作（k=1的情况下）进行替换为另一个操作得到一个新的子模型α~t~，并对α~t~模型执行前向和反向传播，然后更新权重。

  虽然MAGIC-T在一次迭代中只替换一个算子，但它并不仅仅在最初子模型α~0~进行局部采样。事实上，随着t的增大，MAGIC-T可以对不同的架构进行采样，接近于对整个搜索空间的均匀采样。这可以通过观察在架构图G上执行随机游走的MAGIC-T来证明，在G图中，每个架构对应一个节点，当且仅当两个节点在一个算子上不同时，它们之间用边连接，因此G由C^N^个节点组成（C为操作数，N为层数），任意两个节点之间的最短距离为不超过N。假设随机游走{X~t~}~t~从节点X~0~ = α~0~开始，将t时刻节点分布记为π~t~（π~t~为一个C~N~维向量，π~t~(v) = Pr[X~t~ = v]）；另π为G的节点上的均匀分布，通过马尔科夫链随机游走的标准收敛理论可以证明π~t~和π之间的全变差距离（total variational distance）为 $d_{T V} (π_t, π) ≤ exp(−t/N − ln N).$ 因此，如果想要$d_{T V} (πt, π) ≤ ϵ$ ，需要使得 $t ≥ N ln N + N log 1/ϵ$ 。由于t足够大，MAGIC-T的节点分布与均匀采样之间的变差距离足够小，这说明MAGIC-T可以像均匀采样用于从整个搜索空间对不同的架构进行采样。

- MAGIC-A：根据3.2节分析，可以发现使用共享算子的平均输入和输出进行对齐可以减少干扰，然而它增加了C倍的计算成本，且限制了搜索算法的灵活性。因此，可以直接从搜索空间中选择一个表现最好的基模型来对其其他子模型，当一个子模型的性能优于它时将被替换成为基模型。正式地，采样子模型α~t~和基模型α^l^之间的对齐误差定义为：
  $$
  L_{align}(H(α^l), H(α_t)) = \sum\limits_{n=1}^{N} MSE(H^{(α^l)}_n, H^{(α_t)}_n)
  $$
  其中，H~n~是第n层的输出、第n+1层的输入；N为层总数；

  采样得到的子模型α~t~的训练目标是：
  $$
  L = L_{pred} + λL_{align}
  $$
  其中，L~pred~ 是预测值和真实值之间的损失（比如交叉熵损失）；L~pred~为对齐损失；λ是控制对齐损失权重的缩放参数。

  在实践中，可以采样逐块对齐，其中每个块包含几个层，以避免过度正则化。

  MAGIC-A每一步的训练流程如下：

  > - 获取一个批量数据与一个基准子模型α^l^，然后随机采样一个子模型α~t~；
  > - 计算损失，更新模型α~t~的权重；
  > - 如果Val(α~t~) > Val(α^l^)，将α~t~替换成为α^l^（Val(.)是从验证集计算得到的模型准确度）。

## 5. Experiments and Results

### 5.1 Setup

- **搜索空间和超网训练**：

  采用链式超网，候选算子O={MHA12, FFN, CONV3, CONV5}；

  数据集：BookCorpus、English Wikipedia

- **架构搜索和重训练**：使用渐进式收缩方法来搜索有效模型，可以将更多的计算资源分配给有希望的架构，并加快搜索过程。

- **评估**：在GLUE基准上微调预训练模型来评估性能。

### 5.2 Analyses on the Rank Correlation

- **Kendall秩相关性比较：** 首先进行相关分析，评估MAGIC方法是否可以通过减轻干扰来提高子模型的排名。首先，均匀地采样60个子模型，在预训练任务上训练它们，并获得他们在下游MNLI任务上的真实性能（MNLI被广泛用于指示NLP预训练模型的性能）；然后，给定一个超网，可以计算它们的负权重共享验证损失和真实性能之间的Kendall相关系数，如Table 1所示。

  SPOS作为基线模型，在每个训练步骤中随机采样得到一个子模型。

  <img src="assets/6. Analyzing and Mitigating Interference in Neural Architecture Search/image-20230209233058725.png" alt="image-20230209233058725" style="zoom:80%;" />

  结果表明，从两个不同角度提出的MAGIC-T和MAGIC-A能够降低干扰，进一步提高秩相关性。

  <img src="assets/6. Analyzing and Mitigating Interference in Neural Architecture Search/image-20230209233534209.png" alt="image-20230209233534209" style="zoom:150%;" />

- **MAGIC-T分析：**MAGIC-T在每个训练步骤逐步修改k=1个算子，以减少干扰。通过改变k值并度量秩相关来进一步分析MAGIC-T。根据前面分析，更多的拓扑变化（更大的k）会导致对共享算子更严重的干扰，如图3（a）所示，k越大，秩相关性越差，这说明干扰对超网训练是有害的，而所提出的MAGIC-A可以有效地减轻干扰，提高超网的秩能力。

- **MAGIC-A分析：**MAGIC-A选择性能最好的基模型来对其其他子模型的输入和输出，以减少干扰。除了一个性能最佳的基模型，还通过选择不同的基模型进一步分析了该方法。具体地，在每个epoch结束时评估10000个子模型，并选择top p%的子模型作为基模型，然而与表现最好的模型不同，确切的top p%模型变化很大，这导致在不同的epoch选择不同的基模型，使用这样的基模型来对其每个epoch的其他模型会导致超网的不稳定优化。对于稳定的训练，当上一个epoch中选择的基模型脱离当前epoch中top p +- r%(r=10)范围时，我们将替换基模型。用不同的p训练不同的超网，并在图3（b）中呈现排名相关性；图3（c）呈现余弦相似度结果。

  可以看到，一个合适的排名在前30%以内的基模型足以对齐输入和输出并减少干扰，一个更好的基模型可以获得更好的结果，而选择一个表现不佳的模型作为基模型会对超网训练产生负面影响（因为优化的目标是$L_{pred} +λL_{align}$）.

### 5.3 Searching Effective BERT Models

- **GLUE基准测试的结果：** 在两种设置上进行实验：①BERT中的掩码语言模型（MLM）；②ELECTRA提出的RTE（replace token detection）。实验结果如表2所示。

  <img src="assets/6. Analyzing and Mitigating Interference in Neural Architecture Search/image-20230209235128275.png" alt="image-20230209235128275" style="zoom:150%;" />

- **SQuAD数据集上的结果：**进一步评估所搜索得到的架构的泛化性，将其微调为阅读理解任务。

  <img src="assets/6. Analyzing and Mitigating Interference in Neural Architecture Search/image-20230209235505083.png" alt="image-20230209235505083" style="zoom:67%;" />

​	所搜索得到的模型相比于其他基准模型是最优的。

### 5.4 Searching Compressed BERT Models

进一步验证BERT模型压缩算法的通用性，用于对BERT模型进行压缩。

<img src="assets/6. Analyzing and Mitigating Interference in Neural Architecture Search/image-20230209235751382.png" alt="image-20230209235751382" style="zoom:150%;" />

### 5.5 Searching on ImageNet

600M FLOPs约束下，Top1和Top5测试错误率都是最低的。

<img src="assets/6. Analyzing and Mitigating Interference in Neural Architecture Search/image-20230209235923899.png" alt="image-20230209235923899" style="zoom:67%;" />

## 6. Conclusion

本文定量分析了NAS中权重共享干扰的原因，并开发了MAGIC-T和MAGIC-A两种方法来缓解这种干扰，所提出的方法可以提高超网的秩相关性，并可以搜索得到有效的架构。在多种NLP和ImageNet任务上的实验验证了该方法的有效性。

## 7. 其他

### 7.1 论文的研究问题

•NAS中权重共享的方法会引起不同子模型之间的干扰，采样得到的子模型的估计精度和真实精度之间的秩相关性较低，导致训练不足以及性能评估不准确。

  —— 干扰的原因？如何缓解干扰？

### 7.2 研究缝隙

之前的研究工作已经注意到权重共享方法中存在的干扰问题，并通过对比实验发现干扰与搜索空间大小等因素相关，但关于干扰的原因以及如何缓解干扰的讨论很少

### 7.3 论文主要思想

（1）分析干扰的本质：

- 权重共享中不同子模型之间干扰的本质：采样得到的不同子模型共享的算子接收到不同的梯度，导致算子朝着不同的梯度方向进行优化
- 思路：确定可能影响来自不同子模型的梯度差异的因素

（2）提出缓解干扰的方法

### 7.4 思考

- 论文创新点：从梯度优化方向的角度本质上定量分析了NAS权重共享方法中干扰的原因，并且根据所分析得到的思路去设计相关的方法；
- 对于层内也设计搜索空间的情况（即层内涉及多种原子操作的搜索），强制算子输入输出的方式可能会限制层内的搜索过程。