# 语言模型

***reference： 复旦大学邱锡鹏教授***

## 简介

语言模型，即构造模型使其符合自然语言的语法及语义规律。

- 面临的主要问题：早起的形式文法等难以在当前海量语言数据持续增长的背景下满足其建模的需要。

- 解决思路：

  ①尝试通过概率表示对应的单词和文法。即以一个符合语言规律的序列为输入，模型利用序列间关系等特征，输出一个所有词汇上的概率分布。其核心在于**通过概率的大小反映自然语句中文法或者语义的约束**。然而由于单条语句在诸多场景下可能包含巨大的词汇量，导致[传统概率派]所需的计算资源爆炸式增长。

  ②尝试通过条件概率和神经网络的表示方法，在自然语言处理领域预先训练得到大规模的语言模型，并针对具体场景制作特定数据集，供这些大模型在下游任务中通过参数微调以适配具体任务。

BERT之后，不断开始尝试不同的预训练任务、模型架构、训练策略等等；或者继续增大数据流和模型容量来向上探索这一模型的上界。

nlp方面的学界研究被划分为两部分：①致力于在上游训练出性能更为优异的大规模语言模型；②致力于将预训练的大模型转化为面向下游任务场景的合理应用。

New paradigm for NLP：pre-train(预训练)+fine-tuning(微调)。

- 问题：厂商舍弃开源/用户缺乏计算资源。
- 解决方案：①通过人工设计一些模板来匹配大模型；②诸如GPT-3的In-context learning（在上下文中学习），即在应用中只使用大模型前向过程，而不进行计算成本较高的方向过程。

 

基于GPT的方式，下游应用方只需要人工编造少量样例，便可以激活GPT大模型在特定任务上的表现，从而服务于下游的应用，比如使用GPT生成一个网页或者将自然语言转化为数学公式。

##  一种场景——语言模型即服务

随着预训练语言模型的规模急剧增长，出于商业角度考虑和高昂的端侧微调成本，许多大规模预训练语言模型（如GPT-3等）不再被开源，转而以提供API的方式供下游用户在特定场景下进行推理使用，这一场景被称之为**“语言模型即服务”**。

语言模型即服务：①如何构造一个统一的基础模型；②如何高效地调节参数以适应具体的业务场景。

### 1. 统一的基础模型：核心在于尽可能多地囊括语言任务。

依据目标的不同，自然语言处理的任务可以分为七个主要范式：分类、匹配、序列标注、机器阅读理解、Seq2Seq、Seq2ASeq和语言模型范式。而随着下游应用需求的变化，目前主流的三个范式是：机器阅读理解、Seq2Seq和语言模型。

当前，以MLM（掩码语言模型）、In-context learning（在上下文中学习）和Seq2Seq（序列到序列）为代表的处理形式解决了大部分NLP任务。

> 1. MLM：旨在预训练过程中，将一定比例的文本词例屏蔽掉，而模型在训练过程中需要预测被屏蔽的词例；
> 2. In-context learning：指学习从上下文中推理任务的能力。例如，类似GPT-3或者Gopher等大规模语言模型，通过指定对于任务的语言提示，能够直接被用于解决类似文本补全，代码生成以及文本摘要的任务。这种从提示中推理任务的能力通常被称为上下文学习。
> 3. Seq2Seq：即序列到序列，指的是从一个序列生成另外一个序列。该范式涉及两个过程：一是理解前一个序列的内容，二是将理解的内容用于生成新的序列。

eg1. 在自然语言处理中，很多任务并不能转化为序列到序列的方式，比如方面级的情感分析(ABSA)。例如，分析「Drink are always well made」，这句话中有一个评价对象，还有一个评价词以及整体的情感倾向，这些都需要从这个句子中独立抽取出来。事实上，ABSA任务在学界被细分为7个子任务，每个任务由于输入输出的组合不同导致其相对独立。比如有的任务只抽取方面词，有的任务只抽取评价词，不同人物的形式都不一样，所以目前为止没有一个模型能够同时支持在ABSA任务里面所有的子任务。——**邱锡鹏团队提出将ABSA任务安装Seq2Seq的范式进行统一实现**，提出的模型基于一个简单的BART的Encoder-Decoder来构建映射词典，使得算法可以在词典内部进行预测，得到世界上第一个囊括了所有ABSA子任务的模型。

eg2. NER（命名实体识别）也是在自然语言处理中非常重要的一类任务，涵盖了诸多方面的子任务：①连续的 NER：NER 中的词是连续出现的；②嵌入的 NER：在一个实体里面嵌套另外一个实体；③非连续的 NER：一个实体不连续地在正文出现。传统解决方式是依照各个子任务的特点量身定制，采用不同的算法来逐个击破。比如连续的 NER就会采用序列标注的方式，非连续的NER基本上使用转移方法。然而，序列标注很难处理非连续的NER，所以这些方法之间并不通用。

传统的预训练模型有几类代表作，以BERT为例的理解模型，以GPT为代表的生成模型和BART。构造大模型的初衷是为了合众归一,一个新的模型——CPT，其核心思想就是**将理解任务和生成任务合并到一起**，由此提出了一个非对称的Transformer模型结构：CPT。该结构由一个Encoder（任务间共用）和两个Decoder（一个面向理解，一个面向生成）组成，既具备理解的能力又具备生成的能力。此外，由于Decoder网络层次设计得比较浅，使得模型的生成效率提高了2倍以上，在中文数据集上取得了相较于BERT更好的效果。

### 2. 更高效地调节算法

构建起基础的统一预训练模型之后，怎样更加有效地将其迁移到下游的各种具体任务上呢？传统的做法有四种方式：**Text prompt、In-context learning、数据生成，特征学习**。而邱锡鹏团队贡献出两种方法：**即标签调试和黑箱优化。**

> 1. 传统的fine-tuning方法是将Feature Space（特征空间)的参数通过微调映射到Label Space（标签空间），但当模型在大空间向小空间调整的过程中，非常容易发生过拟合的现象。邱锡鹏团队反其道而行之，**将Feature Space固定住，调节Label Space使其向Feature Space靠拢。**标签常用字母“y”来表示，所以这个方法叫做“y-Tuning”。
>
>    
>
>    准确地讲，Y-tuning的模式分为两路进行，一路是预训练模型，另一路才是做标签迁移。这种训练方式是不需要计算预训练语言模型的梯度，但标签迁移部分的梯度仍然是需要调节的。由于模型的参数主要集中于预训练模型中，所以采用这种方式进行Fine-tuning会在很大程度上降低计算量。
>
>    
>
>    邱锡鹏团队之前的一个工作是将文本分类任务转化为文本匹配任务，从而可以引入更多针对于标签的语义信息，来提升面向下游具体任务的效果。同样，团队将这一思想应用到Fine-tuning中，将标签作为网络的输入，与预训练模型获得的特征相结合，可以实现更为显式地建模标签的语义，不需要大模型的梯度，极大地减小了计算开销。实验结果验证，在SuperGLUE数据集上，标签迁移比参数迁移的效果更为突出。
>
> <img src="assets/1. 综述/image-20221212204147048.png" alt="image-20221212204147048" style="zoom:60%;" />
>
> 2. 另一个方法是黑箱优化。整体思想是，**把一个预训练模型部署在服务器端，将其当成一个黑盒子，只提供前项的计算，并通过增加一些Adapt、Prompt来进行参数调节，最终适配到不同的任务上。**在大模型时代，手工设计一些代理任务往往依赖于人工的经验，团队希望能够给大模型以自动调节的能力，而不依赖于手工。
>
>    在现实中，由于prompt的高维度特性，导致无梯度优化的效果很差。而邱锡鹏团队将prompt投射到一个更低维度的空间，在这个空间上进行无梯度的参数优化，再将优化得到的参数逆映射到原始空间。随后，受Prefix Tuning和P-Tuning v2的启发，团队在原始的黑箱优化模型上进行网络层面的加深，希望进一步提升模型的表现，但却引来了参数量大的问题。为此，团队将模型分解为从底层向上逐层的黑箱优化，降低了模型需要训练的参数量，构成了BBTv2模型。在实际的小样本数据集中，这种方法实现的效果可以达到与有梯度的训练模型相媲美。
>
>    
>
>    黑箱调节的意义在于，**对于预训练大模型，使用无梯度方法能够打败基于梯度的方法，赋予了一些大模型更为广阔的应用场景，即只需要前向计算就可以实现特定的下游任务。**

### 3. 语言模型即服务的应用手段，大致可以划分成五类： 

> - **Text prompt：**
>
>   通过人工设计一些基于文本的Prompt，激活大模型面向特定下游任务的能力。但是手工设计偏向于特征工程问题，需要工程师依据既往经验不断调试，十分耗费精力。
>
> - **In-context learning：**
>
>   In-context learning 在GPT模型上展现了良好的表现，但在其他范式的模型上还需要进一步的验证。但是这种手段开辟了一个极具前景的方向，值得学界和工业界继续共同研究。
>
> - **Data generation：**
>
>   不同于直接使用大模型，这种手段是使用大模型生成一定量的数据，再利用生成的数据训练一个小体量的模型，以追求在小样本场景下的应用效果。
>
> - **Black-box optimization：**
>
>   即上文所述的Black-box tuning，让用户根据推理API的返回结果，使用基于搜索的无梯度优化方法自己优化prompt。
>
> - **Feature-based-learning：**
>
>   把预训练模型的输出作为一种 Feature，并结合标签，输入给一些特定的模型，使参数由标签空间向特征空间靠拢，极大地减轻了端侧优化的负担。

**Q：为什么GPT比Seq2Seq更适合做MLM任务？**

A：因为GPT的模型更大，知识容量更高，所以其记住的不仅是语言知识，还有能力记住更多非语言方面的知识。同时这也是我们探索大模型的目的：希望其能够囊括更多的自然语言处理任务。而Seq2Seq范式学习到的更多的是偏向于重构语言的方式，在语义方面的知识学习能力偏弱。