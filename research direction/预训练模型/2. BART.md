---
typora-root-url: ..\..
---

# 【论文精度】生成式预训练模型——BART（**B**idirectional and **A**uto-**R**egressive **T**ransformers）

https://arxiv.org/abs/1910.13461

https://zhuanlan.zhihu.com/p/173858031

对于文本理解任务（Natural Language Understanding），语言预训练模型+下游任务fine-tune基本上已经取得了很好的效果。将BERT等预训练语言模型应用于文本生成任务（Natural Language Generation）时结果并不理想，原因在于预训练阶段和下游任务阶段的差异。

BART是一种符合生成任务的预训练方法，BART，即兼具上下文语境信息和自回归特性的transformer。

## abstract

​	BART是一个预训练的seq2seq的去噪自编码（denoising autoencoder）模型，BART以下方式进行训练**①用任意的噪声函数(noising function)去破坏文本；②学习一个模型来重建原始文本**。它使用一个标准的基于transformer的神经机器翻译架构，可以看作是BERT（双向编码器）、GPT（left-to-right解码器）以及其他预训练方案的推广。文中评估了一系列有噪声方法，通过随机打乱原始句子的顺序和使用新颖的填充方案（in_filling scheme, 其中文本跨度被单个掩码标记替换）来找到最佳性能。BART在针对文本生成进行微调时特别有效，同时也适用于理解任务。

## 1 Introduction

​	自监督方法在许多NLP任务上取得了显著的效果，最成功的方法是掩码语言模型（masked language models）的变体，他们是被训练来重建词被随机掩码的文本的降噪自动编码器。最近研究表明，通过改善masked token 的分布、预测masked token的顺序、以及替换masked token的可用上下文可以获得效益，但是这些方法都专注于特定类型的终端任务，限制了他们的实用性。

​	论文提出了BART模型，是一个结合了双向和自回归transformer的预训练模型，BART是一个适用于非常广的终端任务的用一个seq2seq模型构建的去噪自编码器。预训练分为两个阶段：①用任意的噪声函数(noising function)去破坏文本；②学习一个模型来重建原始文本。

![image-20221216001017297](research direction/预训练模型/assets/2. BART/image-20221216001017297.png)

​	这种设置的一个关键优势是噪声的灵活性，可以对原始文本进行任意转换，包括改变其长度。论文喷壶了一系列噪声化方法，**通过随机打乱原始句子的顺序和使用一种新颖的填充方案来找到最佳性能**，其中任意长度跨度的文本（包括零长度）被一个单一掩码token替换，这种方法通过迫使模型对句子长度进行更多推理（reason），并对输入进行更大范围的转换，推广了BERT算法中原有的单词掩码和下一局预测目标。

> - **GPT**：一种Auto-Regressive(自回归)的语言模型。它也可以看作是Transformer model的Decoder部分，它的优化目标就是标准的语言模型目标：序列中所有token的联合概率。GPT采用的是自然序列中的从左到右（或者从右到左）的因式分解。
> - **BERT**：一种Auto-Encoding(自编码)的语言模型。它也可以看作是Transformer model的Encoder部分，在输入端随机使用一种特殊的[MASK]token来替换序列中的token，这也可以看作是一种noise，所以BERT也叫Masked Language Model。
>
> <img src="research direction/预训练模型/assets/2. BART/image-20221217003733490.png" alt="image-20221217003733490" style="zoom:80%;" />
>
> - **BART**：吸收了BERT的bidirectional encoder和GPT的left-to-right decoder各自的特点，建立在标准的seq2seq Transformer model的基础之上，这使得它比BERT更适合文本生成的场景；相比GPT，也多了双向上下文语境信息。在生成任务上获得进步的同时，它也可以在一些文本理解类任务上取得SOTA。

​	BART开辟了新的思考微调的方式，提出了一种新的机器翻译方案，在该方案中，一个BART模型被堆叠在几个额外的transformer层之上，这些层经过训练基本上将外语翻译为噪声英语，通过BART传播，从而**使用BART作为预训练的目标端（target-side）语言模型**。

​	一些消融分析（ablation analysis）

## 2. Model

BART是一个去噪自动编码器，它将损坏的文档映射到它所集成的原始文档，它被实现为一个带有双向编码器和一个从左到右自回归的解码器的seq2seq模型。对于训练，优化的目标是原始文档的负对数似然。

### 2.1 Architecture

BART使用了标准的seq2seqtransformer架构，在GPT的基础上将ReLU激活函数改为GeLUs并从N(0, 0.02)初始化参数。对于基模型，使用6层编码器和解码器（对于大模型则使用12层）。整体架构跟BERT相近，不同的地方在于：①解码器的每层都在最后隐藏层额外执行了交叉注意（cross-attention）；②BERT在单词预测之前使用了一个额外的前馈网络（feed-forward network），而BART没有。总体来看，BART比同样大小的BERT模型大约多包含10%的参数。

### 2.2 Pre-training BART

BART的训练方法是破坏文档，然后优化重建损失——解码器输出和原始文档之间的交叉熵。与现有的为了特定的去噪方案量身定制的去噪自编码器不同，BART允许我们应用任何类型的文档损坏。在极端的情况下，关于源的所有信息都丢失了，BART就相当于一个语言模型。

> - BERT的这种简单替换导致的是encoder端的输入携带了有关序列结构的一些信息（比如序列的长度等信息），而这些信息在文本生成任务中一般是不会提供给模型的。
> - BART采用更加多样的noise，**意图是破坏掉这些有关序列结构的信息**，防止模型去“依赖”这样的信息。

总结所做的转换（transformation）：

![image-20221216141156985](research direction/预训练模型/assets/2. BART/image-20221216141156985.png)

> - **Token Masking**：与BERT一样，采用随机的token并将其替换为[MASK]
> - **Token Deletion**：输入中的随机tokens被删除。与token masking不同的是，模型必须决定哪些位置是缺失的输入
> - **Text Infilling**：采样一些文本跨度(text spans)，跨度长度取自泊松分布（λ=3），每个跨度被单个[MASK]token取代，0长度的跨度对应[MASK]token的插入。文本填充的灵感来自与Span-BERT，但Span-BERT从不同的分布中采用跨度长度，并将每个跨度替换为完全相同长度的[MASK]token。**文本填充教会模型预测一个跨度中缺失了多少token**.
> - **Sentence Permutation**：以句号(full stop)为单位将文档划分为句子，这些句子进行随机排列
> - **Document Rotation**：均匀随机选择一个token，将文档旋转使得其以该token开始。这将**训练模型识别文档的开始**。
>
> <img src="research direction/预训练模型/assets/2. BART/image-20221217004302664.png" alt="image-20221217004302664" style="zoom:80%;" />

## 3. Fine-tuning BART

BART产生的表示可以以多种方式应用与下游应用(downstream applications)

![image-20221216204508804](research direction/预训练模型/assets/2. BART/image-20221216204508804.png)

### 3.1 Sequence Classification Tasks

​	对于序列分类任务，将相同的输入输入到encoder与decoder，将最终解码器token的最后隐藏层状态输入新的多分类线性分类器。这种方法与BERT中的CLS token有关，BART在尾部添加额外的token，这样解码器中token的表示就可以从完整的输入中关注解码器状态。

### 3.2 Token Classification Tasks

​	这一类问题意思是，将序列的所有token都看作独立的选项，序列长度为M，那么选项的个数就是M，在序列的所有token中选择k个。对于token分类任务，例如SQuAD的答案端点分类(answer endpoint classification)，将完整的文档喂给encoder与decoder，并使用解码器的顶层隐藏状态作为每个词的表示，这个表示被用来对token进行分类。

### 3.3 Sequence Generation Tasks

​	因为BART有一个自回归解码器，因此它可以被直接微调用于序列生成任务，比如抽象问答和摘要，这两个任务中信息都是复制自输入但进行了操作，这与去噪预训练目标紧密相关。在这里，encoder输入为输入序列，decoder自回归地生成了输出。

### 3.4 Machine Translation

​	论文同样探索了使用BART去提升机器翻译译码器，以将输入翻译为英语。先前研究表明**能够通过合并预训练编码器来改进模型，但是在解码器中使用预训练语言模型所获得的收益是有限的**。论文展示了可以使用一整个BART模型（encoder+decoder）作为单个预训练的decoder用于机器翻译是可能的，方法是添加一组从bitext学习到的新的编码器参数。（figure 3b）

​	具体来说，将BART的encoder 嵌入层替换为一个新的随机初始化encoder，该模型是端到端训练的，它训练新的encoder将外文单词映射到BAER可以去噪为英文的输入中，新的encoder可以使用一个与原始BART模型不同的词汇表。

## 4. Comparing Pre-training Objectives

​	与先前工作相比，BART在预训练期间支持更广泛的噪声方案(noising schemes)，使用基本尺寸模型（6个编码器和6个解码器，隐藏大小为768）比较一系列选项，在第5章中将为完整的大规模实验考虑的具有代表性任务子集上进行评估。

### 4.1 Comparison Objectives

​	尽管已经提出了许多预训练目标，但是这些目标之间进行公平地比较一直很困难，至少部分原因是训练数据、训练资源、模型之间的架构差异以及微调程序。论文重新实现最近针对判别(descriminative)和生成任务提出的强预训练方法，目标在于尽可能地控制与预训练目标无关的差异。然而，确实对学习率和层归一化的使用做了一些小的改变（针对每个目标分别进行调整）。作为参考（for reference），将最终实现与BERT发布的结果进行比较，比较了以下方法：

> - **language model**：与**GPT**相似，训练一个自左向右的transformer语言模型，这个模型等价于BART的decoder，无需交叉注意（cross-attention）；
> - **permuted language model**：基于**XLNet**，采样了1/6的tokens，自回归地以一个随机顺序生成它们。为了与其他模型保持一致，没有实现来自XLNet的片段间的相对位置嵌入或注意力。
> - **masked language model**：遵循**BERT**，将15%的token替换为[MASK]符号，然后训练模型独立预测原始token；
> - **multitask masked language model**：与**UniLM**一样，训练一个带有额外自注意力maskd的masked language model，自注意masks按以下比例随机选择：1/6从左到右、1/6从右到左、1/3未掩码、1/3前50%的token未掩码、其余的从左到右掩码。
> - **masked seq-to-seq**：受**MASS**的启发，对包含50%token的跨度进行Mask，并训练序列到序列模型来预测mask token。

​	对于permuted LM、masked LM与multitask masked LM，使用双流注意力(two-stream attention)来高效地计算序列到输出部分的可能性（使用输出上的对角线自注意力掩码来从左到右预测单词）。

​	实验：①将任务视为一个标准的序列到序列问题，其中源输入到encoder，目标为decoder的输出；②将源作为前缀添加到decoder的目标，仅在序列的目标上有损失。实现发现前者更适合BART模型，后者更适合其他模型。

​	为了更直接地比较BART模型对其微调目标建模的能力，在表1中报告了困惑度。

![image-20221216214157466](research direction/预训练模型/assets/2. BART/image-20221216214157466.png)

### 4.2 Tasks

> - **SQuAD** 维基百科段落上的一个抽取式问答任务，答案从给定的文档上下文中抽取文本跨度。与BERT类似，将连接的问题和上下文作为BART编码器的输入，并将它们传递给解码器。该模型包括分类器，用于预测每个token的开始和结束索引。
> - **MNLI**  一个文本分类任务，以预测一个句子是否包含另一个句子，微调后的模型将两个句子和附加的EOS token连接起来，并将它们传递给BART编码器和解码器。与BERT不同，EOS标记被用来对句子之间的关系进行分类。
> - **ELI5** 一个长格式的抽象问答数据集，模型以问题和支持文档的串联为条件生成答案。
> - **XSum** 一个具有高度抽象摘要的新闻摘要数据集。
> - **ConvAI2** 基于上下文和角色的对话响应生成任务。
> - **CNN/DM** 新闻摘要数据集，这里的摘要通常与源句密切相关。

### 4.3 Results

如表1所示，几个趋势很明显：

- 预训练方法在不同任务中的表现差异显著。预训练方法的有效性高度以来与任务。例如，一个简单的语言模型可以获得最优的ELI5性能，但在SQUAD中结果却是最差的。
- token掩码是关键的。基于文档旋转或句子排列的方法在单独的预训练目标表现不好，成功的方法要么使用token删除或掩码，要么使用自注意掩码。在生成任务中，删除似乎优于屏蔽。
- 自左到右预训练提升了生成能力。masked LM与permuted LM在生成任务上表现略差于其他模型，并且是所考虑的模型中唯一两个在预训练期间没有包含自左向右自回归的语言模型。
- 双向编码器对于SQuAD很关键。与前一项工作提出的那样，只有自左到右的解码器在SQuAD上表现很差，因为未来的上下文对于分类决策至关重要，但是BART仅用一半数量的双向层数就获得了相同的性能。
- 预训练目标不是唯一重要的因素。permuted LM表现略逊于XLNet，这种差异的部分原因可能是由于没有包括其他架构的改进，例如相对位置嵌入或段级递归。
- pure LM在ELI5中表现最优。ELI5数据集是一个例外，比其他任务复杂得多，也是唯一一个其他模型的生成性能超过BART的任务，纯语言模型的表现最优，这表明当输出受输入的约束较弱时，BART的效率较低。
- BART取得最稳定的强性能。除ELI5之外，使用文本填充的BART模型在所有任务中都表现良好。

## 5.  Experimental Setup

最近研究表明，当将预训练的批大小以及语料库扩大时，下游性能可以显著提高。为了测试BART在这种状态下的表现如何，并为下游任务创建一个有用的模型，论文使用与BERT相同的规模训练了BART。

### 5.1 Experimental Setup

​	预训练了一个encoder与decoder分别有12层的大模型，隐藏大小为1024.与RoBERTa一样，使用批大小为8000，训练500000步数。文档使用与FPT-2相同的字节对编码进行标记。基于第4章的结果，**使用文本填充与句子排列的组合方法，对每个文档掩码30%的token，对所有句子进行排列**。尽管句子排列仅在CNN/DM摘要数据集上表现出显著增益，我们假设更大的预训练模型会表现更有。为了帮助模型更好的拟合数据，**在最后的10%训练步中禁用了dropout**。使用的数据包含160Gb的新闻、书籍、股市和网页文本。

### 5.2 Discriminative Tasks

![image-20221216234204919](research direction/预训练模型/assets/2. BART/image-20221216234204919.png)

总体来看，BART与RoBERTa表现相似，在大部分任务上只有很小的差别。

结论：BART在生成任务上的提升并不以牺牲分类性能为代价。

### 5.3 Generation Tasks

对于文本生成任务，对BART作为从输入到输出文本的标准序列到系列模型进行了微调，使用标签平滑交叉熵损失(label smoothed cross entropy loss)，平滑参数设置为0.1.在生成过程中，将波束大小设置为5，在波束搜索中删除重复的trigrams，并在验证集上使用min-len、max-len、长度惩罚来调整模型。

- **Summarization** （摘要）![image-20221216235011623](research direction/预训练模型/assets/2. BART/image-20221216235011623.png)
  - CNN/DailyMail的摘要往往与源句子相似，抽取模型在这个任务表现较好，BART优于现有工作。
  
  - XSum具有高度抽象性，抽取模型表现较差。
  
- **Dialogue** （对话）

![image-20221216235857774](research direction/预训练模型/assets/2. BART/image-20221216235857774.png)

​			在ConvAI2上进行评估，智能体必须根据之前的上下文和文本指定的任务角色生成相应，			BART在两个自动化指标上优于先前工作。

- **Abstractive QA**  （抽象问答）

<img src="research direction/预训练模型/assets/2. BART/image-20221217000046062.png" alt="image-20221217000046062.png" style="zoom:67%;" />

​			使用ELI5数据集来测试模型的生成长格式自由答案的能力。

### 5.4 Translation

<img src="research direction/预训练模型/assets/2. BART/image-20221217000211334.png" alt="image-20221217000211334" style="zoom:67%;" />

在WMT16罗马-英语翻译任务上评估性能。使用6层transformer源编码器将罗马文映射到BART能够去噪到英文的一种表示。

## 6 Qualitative Analysis

​	BART在摘要指标上有了很大的改进，比之前的最先进技术提高了6个点。为了了解BART在自动化指标之外的表现，对其进一步定性分析。

​	模型输出高度抽象，通常也是事实准确的。

总结：BART预训练学习了自然语言理解和生成的强大结合。

## 7 Related Work

​	早起的预训练方法是基于语言模型的，GPT仅建模左上下文，这对于一些任务来说是有问题的；ELMo连接了只左和只右的表示，但没有预训练这些特征之间的交互；Radford等人证明了非常大的语言模型可以作为无监督的多任务模型。

​	BERT引入了masked LM，允许预训练学习左右上下文单词之间的交互。最近工作表明，可以通过更长时间的训练、通过跨度绑定参数、以及通过掩码跨度而不是单词来实现非常强的性能。预测不能自回归降低了BERT算法在生成任务中的有效性。

​	UniLM使用掩码组合微调BERT，其中一些只允许左侧上下文。与BERT一样，这允许UniLM用于生成和判别任务，不同之处在于，UniLM预测是有条件独立的，而BART预测是自回归的，BART减少了预训练任务和生成任务之间的不匹配，因为解码器总是在未损坏的上下文上进行训练。

​	MASS可能是与BART最相似的模型，一个连续的token被掩码的输入序列被映射到一个由缺失的token组成的序列。MASS对于判别任务不太有效，因为编码器和解码器中会输入不相交的token集。

​	XL-Net通过以打乱的顺序自回归地预测masked token实现对BERT的扩展，这个目标允许预测在左右上下文中的条件；相比之下，BART解码器在训练前从左到右工作，在生成时匹配设置。

## 8 Conclusions

​	论文介绍了BART，一个学习将被破坏的文档映射为原文档的预训练方法。BART在判别任务(discriminative task)取得了与RoBERTTa相近的性能，在文本生成任务上取得了SOTA性能。未来工作将探索更多的破坏文档的方法，也许可以将其定制为特定的最终任务。
