# pytorch使用记录

## 1. nn.Embedding

> A simple lookup table that stores embeddings of a fixed dictionary and size.

> Parameters
>
> - num_embeddings (int) – size of the dictionary of embeddings
> - embedding_dim (int) – the size of each embedding vector
> - padding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient; therefore, the embedding vector at padding_idx is not updated during training, i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.
> - max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm.
> - norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.
> - scale_grad_by_freq (boolean, optional) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False.
> - sparse (bool, optional) – If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.
>
> 
>
> Variables
>
> **~Embedding.weight** ([*Tensor*](https://pytorch.org/docs/1.8.1/tensors.html#torch.Tensor)) – the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)
>
> 
>
> Shape:
>
> - Input: (∗) , IntTensor or LongTensor of arbitrary shape containing the indices to extract
> - Output:(∗,*H*) , where * is the input shape and H=embedding_dim

