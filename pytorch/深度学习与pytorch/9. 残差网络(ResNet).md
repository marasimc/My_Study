# 深度学习-残差网络(ResNet)

*reference: [深度学习之16——残差网络(ResNet) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/72679537)*

## 1. 为什么要引入残差网络

### 1.1 网络深度为什么重要

```
以CNN网络为例，输入的是图片矩阵，也是最基本的特征，整个CNN网络就是一个详细提取的过程，从底层特征逐渐提取到高度抽象的特征，网络层数越多意味着能够提取到的不同级别的抽象特征月丰富，并且越深的网络提取的特征月抽象，就越具有语义信息。
```

### 1.2 为什么不能简单地增加网络层数

```
对于传统的CNN网络，简单地增加网络深度容易导致梯度消失或爆炸，针对梯度消失和爆炸的解决办法一般是正则初始化(normalized initialization)和中间的正则化层(intermediate normalization layers)，但是这会导致另一个问题，退化问题，随着网络层数的增加，在训练集上的准确率却饱和甚至下降了。这个和过拟合不一样，因为过拟合在训练集上的表现会更加出色。

按照常理更深层的网络结构的解空间是包括浅层的网络结构的解空间的，也就是说深层的网络结构能够得到更优的解，性能会比浅层网络更佳。但是实际上并非如此，深层网络无论从训练误差或是测试误差来看，都有可能比浅层误差更差，这也证明了并非是由于过拟合的原因。导致这个原因可能是因为随机梯度下降的策略，往往解到的并不是全局最优解，而是局部最优解，由于深层网络的结构更加复杂，所以梯度下降算法得到局部最优解的可能性就会更大。
```

### 1.3 如何解决退化问题

```
保留深层网络的深度，又可以有浅层网络的优势 -> 如果将深层网络的后面若干层学习成恒等映射h(x) = x ，那么模型就退化成浅层网络
因此将网络设计成 h(x) = f(x) + x，只要f(x) = 0就构成了一个恒等映射h(x) = x，这里f(x)为残差。
```

![1655533368255](image/9. 残差网络(ResNet)/1655533368255.png)

```
Resnet提供了两种方式来解决退化问题：identity mapping以及residual mapping。identity mapping指的是图中“弯线”部分，residual mapping指的是非“弯线”的剩余部分。 f(x)是求和前网络映射，h(x)是输入到求和后的网络映射。
```

```
假设有个网络参数映射： g(x) 和 h(x) ，这里想把5映射成5.1，那么 g(5) = 5.1，引入残差的映射 h(5)  = f(5) + 5 = 5.1, f(5) = 0.1。引入残差的映射对输出的变化更加敏感，比如从输出的5.1再变化到5.2时，映射 g(x) 的输出增加了1/51=2%。而残差结构输出的话，映射 f(x) 从0.1到0.2，增加了100%。明显后者的输出变化对权重的调整作用更大，所以效果更好。
```

## 2. 残差块

```python
def conv3x3(in_channel, out_channel, stride=1):
    return nn.Conv2d(in_channel, out_channel, 3, stride=stride, padding=1, bias=False)

def conv1x1(in_channel, out_channel, stride=1):
    return nn.Conv2d(in_channel, out_channel, 1, stride=stride)

class residual_block(nn.Module):
    '''
        定义残差块
    '''
    def __init__(self, in_channel, out_channel):
        super(residual_block, self).__init__()
        stride = 1 

        self.conv1 = conv3x3(in_channel, out_channel, stride = stride)
        self.bn1 = nn.BatchNorm2d(out_channel)      # 归一化处理

        self.conv2 = conv3x3(out_channel, out_channel)
        self.bn2 = nn.BatchNorm2d(out_channel)
        
        self.conv3 = conv1x1(in_channel, out_channel, stride = stride)
        # self.conv3 = nn.Conv2d(in_channel, out_channel, 3, stride = 1, padding=1)

    def forward(self, x):
        out = self.conv1(x)
        out = F.relu(self.bn1(out), True)
        out = self.conv2(out)
        # out = F.relu(self.bn2(out), True)
        out = self.bn2(out)

        x = self.conv3(x)
        return F.relu(x + out, True)
```

