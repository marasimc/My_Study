# pytorch自动求导机制

```python
import torch
from torch.autograd import Variable
```

## 1. 简单情况的自动求导

注：.backward()默认只会对当前Variable下的 leaf Tensor 进行自动求导

```python
x = Variable(torch.Tensor([2]), requires_grad=True)
y = x + 2
z = y ** 2 + 3
print(z)       # tensor([19.], grad_fn=<AddBackward0>)

# 使用自动求导(z对x的导数)
z.backward()
# 得到 x 的梯度
print(x.grad)  # tensor([8.])
```

```python
x = Variable(torch.randn(10, 20), requires_grad=True)
y = Variable(torch.randn(10, 5), requires_grad=True)
w = Variable(torch.randn(20, 5), requires_grad=True)

out = torch.mean(y - torch.matmul(x, w)) # torch.matmul 是做矩阵乘法
out.backward()
# 得到 x 的梯度
print(x.grad)
# 得到 y 的的梯度
print(y.grad)
# 得到 w 的梯度
print(w.grad)
```

## 2. 复杂情况的自动求导

对一个向量或者矩阵自动求导

```python
m = Variable(torch.FloatTensor([[2, 3]]), requires_grad=True) # 构建一个 1 x 2 的矩阵
n = Variable(torch.zeros(1, 2)) # 构建一个相同大小的 0 矩阵
print(m)
print(n)

# 通过 m 中的值计算新的 n 中的值
n[0, 0] = m[0, 0] ** 2
n[0, 1] = m[0, 1] ** 3
print(n)

# 在 PyTorch 中，如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和 n 一样大，比如是(w0, w1)
n.backward(torch.ones_like(n)) # 将 (w0, w1) 取成 (1, 1)
print(m.grad)
```

## 3. 多次自动求导

通过调用 backward 我们可以进行一次自动求导，如果我们再调用一次 backward，会发现程序报错，没有办法再做一次。这是因为 PyTorch 默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西

```python
x = Variable(torch.FloatTensor([3]), requires_grad=True)
y = x * 2 + x ** 2 + 3
print(y)

y.backward(retain_graph=True) # 设置 retain_graph 为 True 来保留计算图
print(x.grad)   # tensor([8.])
y.backward() 	# 再做一次自动求导，这次不保留计算图
print(x.grad)   # tensor([16.]), 可以发现 x 的梯度变成了 16，因为这里做了两次自动求导，所以讲第一次的梯度 8 和第二次的梯度 8 加起来得到了 16 的结果。
```

