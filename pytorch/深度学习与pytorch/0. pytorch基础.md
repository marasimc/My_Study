# pytorch基础

## 1. Tensor 

### 1.1 pytorch 与 numpy的转换

PyTorch 的官方介绍是一个拥有强力GPU加速的张量和动态构建网络的库，其主要构件是张量，所以我们可以把 PyTorch 当做 NumPy 来用，PyTorch 的很多操作好 NumPy 都是类似的，但是因为其能够在 GPU 上运行，所以有着比 NumPy 快很多倍的速度。

```python
import torch
import numpy as np

# 创建一个 numpy ndarray
numpy_tensor = np.random.randn(10, 20)

# 1.将numpy的ndarray转换到tensor上
pytorch_tensor1 = torch.Tensor(numpy_tensor)
pytorch_tensor2 = torch.from_numpy(numpy_tensor)

# 2.将 pytorch tensor 转换为 numpy ndarray
# 2.1如果 pytorch tensor 在 cpu 上
numpy_array = pytorch_tensor1.numpy()

# 2.2如果 pytorch tensor 在 gpu 上
numpy_array = pytorch_tensor1.cpu().numpy()
```

### 1.2 Tensor的使用

```python
''' 将 Tensor 放到 GPU 上 '''
# 第一种方式是定义 cuda 数据类型
dtype = torch.cuda.FloatTensor # 定义默认 GPU 的 数据类型
gpu_tensor = torch.randn(10, 20).type(dtype)

# 第二种方式更简单，推荐使用
gpu_tensor = torch.randn(10, 20).cuda(0) # 将 tensor 放到第一个 GPU 上
gpu_tensor = torch.randn(10, 20).cuda(1) # 将 tensor 放到第二个 GPU 上


''' 将 tensor 放回 CPU  '''
cpu_tensor = gpu_tensor.cpu()
```

```python
''' 访问到 Tensor 的一些属性 '''
# 1.可以通过下面两种方式得到 tensor 的大小
print(pytorch_tensor1.shape)      # torch.Size([10, 20])
print(pytorch_tensor1.size())     # torch.Size([10, 20])

# 2.得到 tensor 的数据类型
print(pytorch_tensor1.type())     # torch.FloatTensor

# 3.得到 tensor 的维度
print(pytorch_tensor1.dim())      # 2

# 4.得到 tensor 的所有元素个数
print(pytorch_tensor1.numel())    # 200
```

```python
# 指定tensor的数据类型
pytorch_tensor1 = pytorch_tensor1.type(torch.DoubleTensor)
```

### 1.3 Tensor的操作

```python
x = torch.ones(2, 2)
print(x)        # 这是一个float tensor
print(x.type()) # torch.FloatTensor

# 将其转化为整型
x = x.long()
# x = x.type(torch.LongTensor)
print(x)

# 再将其转回 float
x = x.float()
# x = x.type(torch.FloatTensor)
print(x)

# 沿着行取最大值
max_value, max_idx = torch.max(x, dim=1)   # max_value -> 每一行的最大值（torch.FloatTensor）; max_idx -> 每一行最大值的下标（torch.LongTensor）

# 沿着行对 x 求和
sum_x = torch.sum(x, dim=1)
print(sum_x)

# 增加维度或者减少维度
print(x.shape)      # torch.Size([4, 3])
x = x.unsqueeze(0)  # 在第一维增加
print(x.shape)      # torch.Size([1, 4, 3])
x = x.unsqueeze(1)  # 在第二维增加
print(x.shape)      # torch.Size([1, 1, 4, 3])
x = x.squeeze(0)    # 减少第一维
print(x.shape)      # torch.Size([1, 4, 3])
x = x.squeeze()     # 将 tensor 中所有的一维全部都去掉
print(x.shape)      # torch.Size([4, 3])
```

```python
x = torch.randn(3, 4, 5)
print(x.shape)         # torch.Size([3, 4, 5])

# 使用permute和transpose进行维度交换
x = x.permute(1, 0, 2) # permute 可以重新排列 tensor 的维度
print(x.shape)         # torch.Size([4, 3, 5])

x = x.transpose(0, 2)  # transpose 交换 tensor 中的两个维度
print(x.shape)         # torch.Size([5, 3, 4])
```

```python
# 使用 view 对 tensor 进行 reshape
x = torch.randn(3, 4, 5)
print(x.shape)         # torch.Size([3, 4, 5])

x = x.view(-1, 5) 	   # -1 表示任意的大小，5 表示第二维变成 5
print(x.shape)         # torch.Size([12, 5])

x = x.view(3, 20)      # 重新 reshape 成 (3, 20) 的大小
print(x.shape)         # torch.Size([3, 20])
```

```python
x = torch.randn(3, 4)
y = torch.randn(3, 4)

# 两个 tensor 求和
z = x + y
# z = torch.add(x, y)
```

```python
'''
另外，pytorch中大多数的操作都支持 inplace 操作，也就是可以直接对 tensor 进行操作而不需要另外开辟内存空间，方式非常简单，一般都是在操作的符号后面加_
'''
x = torch.ones(3, 3)
print(x.shape)        # torch.Size([3, 3])

# unsqueeze 进行 inplace
x.unsqueeze_(0)       
print(x.shape)        # torch.Size([1, 3, 3])

# transpose 进行 inplace
x.transpose_(1, 0)
print(x.shape)        # torch.Size([3, 1, 3])


x = torch.ones(3, 3)
y = torch.ones(3, 3)
print(x)

# add 进行 inplace
x.add_(y)
print(x)
```

## 2. Variable

tensor 是 PyTorch 中的完美组件，但是构建神经网络还远远不够，我们需要能够构建计算图的 tensor，这就是 Variable。Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性：Variable 中的 tensor本身`.data`，对应 tensor 的梯度`.grad`以及这个 Variable 是通过什么方式得到的`.grad_fn`

```python
# 通过下面这种方式导入 Variable
from torch.autograd import Variable
```

```python
x_tensor = torch.randn(10, 5)
y_tensor = torch.randn(10, 5)

# 将 tensor 变成 Variable
x = Variable(x_tensor, requires_grad=True) # 默认 Variable 是不需要求梯度的，所以我们用这个方式申明需要对其进行求梯度
y = Variable(y_tensor, requires_grad=True)


z = torch.sum(x + y)
print(z.data)     # -2.1379 [torch.FloatTensor of size 1]
print(z.grad_fn)  # <SumBackward0 object at 0x10da636a0>  -> 说明了z是通过sum这种方式得到的

# 求 x 和 y 的梯度，使用了pytorch提供的自动求导机制
z.backward()
print(x.grad)
print(y.grad)
```

