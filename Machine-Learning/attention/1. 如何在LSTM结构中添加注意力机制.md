# 在LSTM结构中添加注意力机制

- 通过在每个时间步引入一个自注意力机制，计算当前时间步与之前时间步的相似度，进而得到一个加权表示的上下文向量，这个向量可以与当前时间步的输入进行加和，得到一个新的输入向量。这样，模型可以更加灵活地利用历史信息来生成当前的输出。
- 多头注意力机制是一种更强大的注意力机制，可以将一个序列分别映射到多个向量空间中并在这些空间中进行注意力计算，最后将结果进行拼接，得到一个更加丰富的上下文表示。