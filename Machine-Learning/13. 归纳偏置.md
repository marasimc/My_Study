# 归纳偏置（Inductive Biases）

> reference:  [[2011.15091\] Inductive Biases for Deep Learning of Higher-Level Cognition (arxiv.org)](https://arxiv.org/abs/2011.15091)

## 1. 当前机器学习算法的局限

- 当前机器学习系统能够利用大量带标签样本的监督学习（supervised learning）或基于频繁奖励信号的强化学习（reinforcement learning）在特定的狭窄的任务中取得优异的性能，这要求算法为每一个特定的任务学习一套独立的参数，当任务分布发生变化时，这些方法通常不具有鲁棒性。

针对以上问题，目前机器学习领域的一个研究方向是利用多个数据集训练模型，**每个数据集为模型提供了不同的视角**，这种方法被称为==多任务学习（multi-task learning）==。

尽管多任务学习采用不同的数据集使得模型能够适应多个分布，但是否有更好的方式使得模型能够在一个全新的任务或分布中，通过零样本的分布外泛化（zero-shot out-of-distribution generalization）或小样本的迁移学习（transfer learning），取得优异性能？

—— 对不同任务或不同分布之间的联系做出新的假设，让模型更好地做到分布外泛化



- 当前机器学习的一个可能的发展方向：跳出原有的**统计学习框架（statistical learning framework）**，利用先验知识对模型结构进行限制，通过这种方式提炼出任务之间的某种共性，从而能够**基于小样本实现任务间的快速泛化**。

- 相较于将数据当做是服从统一分布的一系列独立样本，我们可以从真实世界发展规律的视角去反映数据的本质，然后利用对问题或任务或数据的理解去构造满足某种特征的人工智能模型或算法，这种对于模型的假设被称为==**归纳偏置（inductive biases）**==。

## 2. 归纳偏置

​		机器学习中的**no-free-lunch定理**表明对于任意函数而言，一定的偏好（或者说归纳偏置）是实现泛化的必要手段，也就是说，**没有完全通用的学习算法，任何学习算法都只能在特定的一些分布上实现泛化**。一般来说，给定特定的数据集和损失函数，一个学习算法存在许多可行解。然而，**给定有限的训练集，要想在新数据上实现泛化就必须依赖于对最优解做出的合理假设**。因此，==为了让人工智能达到与人类相近的性能，我们**需要依据某些直觉和经验，针对不同问题探索合适的归纳偏置，使得学习算法能够更好地搜寻到具有某种特性的解**。==

例子：

- CNN的归纳偏置是局部性（locality），空间不变性（spatial invariance），平移等效性（translation equivariance），即卷积核的参数共享；
- RNN的归纳偏置是时序性（sequential），时间不变性（time invariance），即时间步上的参数共享；
- GNN是节点和关系的不变性；
- attention是置换不变性。

​		

引入归纳偏置的方式

- 在目标函数中引入适当的正则化，对网络结构进行限制，参数共享，选择合适的优化方法，对已知变化的不变性和等效性
- 在一个贝叶斯模型找那个选择合适的先验分布。例如，我们可以将矩阵相乘替换成卷积和池化
- 对输入特征做平均
- 利用具有数据样本经过变化之后的数据集训练，实现神经网络输出的平移不变性

## 3. 归纳偏置与数据的关系

可以将归纳偏置或先验知识看做是**“变相的训练数据”（“training data in disguise”）**，同样也可以利用更多的数据来弥补先验归纳偏置的不足。

## 4. 迁移学习、持续学习、元学习

## 5. 总结

​    	归纳偏置实际上就是利用对问题、任务或者数据的理解对模型进行一定的先验假设。**对于特定分布的数据而言，合理的归纳偏置能够缩小参数的搜索范围，从而能够利用更少的数据更快更高效地得到最优解**；对于不同分布的数据，合理的归纳偏置还能够反映不同任务之间的共性，从而帮助模型快速适应不同的任务，实现分布外泛化。这反映了深度学习从==深度统计模型（deep statistical models）到深度结构化模型（deep structural models）==的发展趋势。在这种情况下，我们需要思考的是如何利用先验知识构建合理的归纳偏置，并将其应用到神经网络结构以及训练框架的设计中。