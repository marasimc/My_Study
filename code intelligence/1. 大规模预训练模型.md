# 大规模预训练模型

![img](https://pic1.zhimg.com/80/v2-0af283fb998ac1811df55df4c3b88580_720w.webp)

从模型输入来讲，与自然语言预训练模型相比，编程语言预训练模型的输入不仅包括自然语言（代码注释），还包括代码（Python/C/C++/JAVA等），这就导致自然语言预训练模型很难直接泛化到编程语言，需要研究专门的编程语言预训练模型。

## 代码智能中的预训练模型

[分享会(1): 代码智能中的预训练模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/586946481)

代码智能包括代码理解、代码生成、意图预测等问题，现实的应用包括代码补全、代码翻译、代码摘要等应用。代码的数据形式有很多，包括源代码、code diff、汇编代码等等，而我们一般讨论的代码预训练主要针对的是python、java代码等这类源代码，常见的数据集包括CodeSearchNet、bigcode、the stack等等，代码预训练的核心思想就是把代码当成Token序列。

### 1. 代码理解与代码生成

**CodeBERT**：将BERT迁移到代码预训练领域，核心思想还是“把代码当作文本”。预训练数据包括代-文本数据和纯代码数据，代码-文本数据中的文本主要来自于代码对应的文档解释。模型结构和BERT保持完全一致，具体使用的是Base规模的模型，初始化则使用RoBERTa-Base。

**GraphCodeBERT**：比CodeBERT最大区别就是加入了AST信息（Abstract Syntax Tree）。AST可以通过现成工具解析出来，这部分信息加进去有助于模型理解代码的结构。并且在模型的具体设计上，通过设计mask的方式让AST上的节点做attention的时候不是做全局attention，或者换个角度理解，就是加入AST信息的同时，还借助AST的先验知识加入规则，影响attention mask。

### 2. 特殊场景的代码预训练



### 3. 代码预训练大模型





## ROBERTa



## CodeBERT: a Pre-Trained Model for Programming and Natural Languages

[代码也能预训练，微软&哈工大最新提出 CodeBERT 模型，支持自然-编程双语处理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/108243161)

将BERT应用到了Python、PHP、Java、JavaScript、Go、Ruby等编程语言的代码搜索和生成任务当中。这篇论文提出了一个被称为「CodeBERT」的双模预训练模型，据作者介绍，这也是目前已知的第一个大型 NL-PL（自然语言-编程语言）预训练模型。该预训练模型能够处理NL-PL 的普遍问题，例如用自然语言搜索代码、自动生成代码等。

CodeBERT有一大亮点，即尽管它只在Ruby、JavaScript、Go、Python、Java、PHP等代码语言上进行了预训练，但预训练的模型却可以泛化到其他代码语言任务上，例如C#语言。

### 1. 背景

双模态预训练模型：兼顾NLP任务和Python、java等编程语言。具体来说，CodeBERT抓住了自然语言和编程语言之间的语义联系，能够支持自然语言代码搜索等NL-PL理解任务以及一系列像代码生成这样的生成任务。

为了利用Nl-PL（自然语言-编程语言）对的双模实例（bimodal instances）以及大量可用的单模代码（unimodal codes），作者使用了**混合目标函数**来训练CodeBERT，包括标准掩码语言建模和可替换Token检测。

### 2. 框架

在预训练阶段，总共设计了两部分输入，一个是自然语言文本，另一个是编程语言的代码。对于自然语言文本将其视为单词序列，并拆分为WordPiece。对于编程代码，将其看做Token序列。

CodeBERT的输出也包括两个部分：1、聚合序列表示；2、有标记的上下文向量（contextual vector）。

训练CodeBERT所使用的数据集是Husain等人在2019年提供的最新数据集，里面包括 2.1M双模数据和6.4M 单码数据，其中**双模码数据是指自然语言-代码对的并行数据，单码是指“未成对”的数据。**

在模型训练的设计上，其主要包括两个目标，其一是掩码语言建模，其二是可替换Token检测。在第二个目标中，作者进一步使用了大量的单模码数据。

> 目标一：掩码语言建模。将NL-PL对作为输入，随机为NL和PL选择位置进行掩码，然后用特殊的掩码Token进行替换。注意，掩码语言建模的任务是预测出被掩码的原始Token。
>
> 目标二：替换Token检测。在这部分有两个数据生成器，分别是NL生成器和PL生成器，这两个生成器都用于随机掩码位置集（randomly masked positions）生成合理的备选方案。另外，还有一个学习生成器用来检测一个词是否为原词，其背后原理是一个二进制分类器，这里与GAN不同的是，如果生成器碰巧产生正确的Token，则该Token的标签是“real”而不是“fake”。

模型训练的最后一步是模型微调，具体操作是在NL-PL任务中使用不同的CodeBERT设置。例如在自然语言代码搜索中，会使用与预训练阶段相同的输入方式。而在代码到文本的生成中，使用编码器-解码器框架，并使用CodeBERT初始化生成模型的编码器。

### 3. 实验

> 1）将CodeBERT应用到自然语言代码搜索任务上，并与传统方法进行对比；
>
> 2）进行NL-PL Probing实验，考察CodeBERT在预训练阶段到底学习了什么知识；
>
> 3）将CodeBERT应用到生成任务当中；
>
> 4）考察CodeBERT预训练模型的泛化能力，发现效果非常之好。

#### 3.1 自然语言代码搜索

#### 3.2 NL-PL Probing

NL-PL Probing的目标是测试模型的正确预测能力

#### 3.3 代码文档生成

#### 3.4 泛化能力



## PLBART