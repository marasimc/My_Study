# 大模型推理能力提升策略

## 1. 思维提示链 [chain of thought prompting]

reference: [思维链（Chain-of-thoughts）作为提示 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/493533589)

背景：提示学习（prompt learning）浪潮兴起，离散式提示学习（提示词的组合）+ 连续化提示学习（冻结大模型权重+微调较小参数达到等价性能）。连续化提示学习其中的一些好处伴随的一些局限性，比如伪资源节约，不稳定等等。

### 1.1 相关文章

[https://github.com/Timothyxxx/Chain-of-ThoughtsPapers](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers)

### 1.1.1 Chain-of-Thought Prompting Elicits Reasoning in Large Language Models —— 思维提示连概念的开山之作。

现在语言模型的规模越来越大，但是即便是现在最大的语言模型，它们也往往很难在涉及到推理方面的任务取得很好的表现，也就是说，他们通常很难在数学，符号，以及常识的推理上取得尚佳的表现。

这篇文章主要是针对大语言模型在遇到语言推理任务时的局限性，提出了 chain of thought，也就是思维链。CoT 的定义：人类在遇到一系列问题时所产生的推理步骤，而它们的表现形式就是一系列的短句子（比如说在背景介绍中所提到的遇到数学问题时所产生的中间推理步骤）

> 在此前关于大规模语言模型的推理任务中，有两种方法：
>
> 1. 针对下游任务对模型进行微调；
> 2. 为模型提供少量的输入输出样例进行学习。
>
> 但是这两种方法都有着局限性，前者微调计算成本太高，后者采用传统的输入输出样例在推理任务上效果很差，而且不会随着语言模型规模的增加而有实质性的改善。

> 语言模型的规模达到 100B 的参数量之后，就能够在像 sentiment analysis and topic classification 这种分类任务上取得非常好的结果，作者将这类任务归纳为 system-1，也就是能够人类很快很直观地理解的任务；还有一类任务需要很慢而且是很仔细的考虑，作者将其归纳为 system-2 （比如一些设计逻辑、常识的推理任务），作者发现，即便语言模型的规模达到了几百B的参数量，也很难在 system-2 这类任务上获得很好的表现（作者将这种现象称为flat scaling curves）。

简单来说，思维链是一种离散式提示学习，更具体地，大模型下的上下文学习（即不进行训练，将例子添加到当前样本输入的前面，让模型一次输入这些文本进行输出完成任务），相比于之前传统的上下文学习，即通过x1,y1,x2,y2,....x_test作为输入来让大模型补全输出y_test，思维链多了中间的一些闲言碎语絮絮叨叨，以下面这张图为例子：

<img src="https://pic3.zhimg.com/80/v2-e1b5adff46170f633e8ed635e7a57646_720w.webp" alt="img" style="zoom:50%;" />

思维链的絮絮叨叨即不直接预测y，而是将y的“思维过程”r（学术上有很多学者将这种过程统称为relationale）也要预测出来。当然最后我们不需要这些“思维过程”，这些只是用来提示获得更好的答案，只选择最后的答案即可。作者对不同的数据集的原本用于上下文学习的提示标注了这些思维链然后跑了实验，发现这么做能够显著的提升性能（左图），且这种性能的提升是具有类似于井喷性质（右图）的（后来他们发文号称这种性质叫涌现性，我们这里先按下不表）。（CoT 与 Standard prompting 唯一的区别就是，CoT 在样例中在给出问题的同时，不仅给出了答案，在答案之前还给出了人为写的中间推理步骤）

![img](https://pic1.zhimg.com/80/v2-a5e20815867c458a74b5e30a13f3b53c_720w.webp)

> 1. CoT 原则上能够让模型把一个多步的问题分解出各种中间步骤，使那些具有更多推理步的问题有机会分配到更多的计算量（如果是从最后的将拼接好的问题、答案样例以及所要求解的问题和前缀输入到语言模型中产生最后的答案这一步来看，对于一个更难的问题，在续写的时候，CoT就使得语言模型能够产生更多的中间推理步骤，因为语言模型在生成输出的时候是一个一个 token 进行生成的，那么如果问题越难，CoT 又使得生成的中间步骤越多，那么整体上生成的 token 的数量也会越多，自然而然在求解更难的问题的时候就会使用到更多的计算量。就好比人类在遇到更难得问题的时候，可能就会耗费更多的脑力，这样 CoT 也能够让计算机能够对更难的问题分配更多的计算资源）
> 2. CoT 提供了可解释性，也就是在不知道答案的情况下，也能够知道答案是怎样得来的，也就是所谓的中间推理步骤
> 3. 作者认为 CoT 在原则上能够适用于任何人类能够用语言所能解决的问题，而不仅仅是数学、逻辑、常识这类的问题。因为 CoT 本身的载体就是一系列的短句子，本身也是人类语言
> 4. 当一个语言模型训练好之后，就能够通过 few-shot prompting 这种范式，在每个样例中写上中间推理步骤，再拼接好所要求解的问题输入到语言模型，就能够引发语言模型续写中间推理步骤，再得出最后的答案（像 Zero-Shot CoT 就发现，甚至都不需要在 few-shot 这些样例中添加 CoT ，可以仅凭“let's think step by step”作为 CoT 的推理；而 Auto CoT ，也就是“Let's think not just step by step but one by one”使用了多个“let's think step by step”就可以自动地构造 few-shot 的样例，从而弥补了 Zero-shot 和 Few-shot 之间的性能差异）

- 在大规模语言模型上使用 CoT 推理使得在实际应用中服务的成本很高；进一步的研究可以探索如何在较小的模型中进行推理

### 1.1.2 Self-Consistency Improves Chain of Thought Reasoning in Language Models——多数投票显著提高CoT性能

这篇文章几乎用的和初代思维链文章完全一样的数据集和设置，主要改进是使用了对答案进行了多数投票（majority vote），并且发现其可以显著地提高思维链方法的性能。

![img](https://pic1.zhimg.com/80/v2-82f1234d53f8c0623d783eccdff272e4_720w.webp)

### 1.1.3 STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning——提出了一种boost方法，让中小模型也可以通过训练具有思维链能力





## 2. 思维提示链与自洽性 [self-consistency]相结合

论文地址：https://arxiv.org/pdf/2203.11171.pdf

复杂的推理任务通常有多个能得到正确答案的推理路径，自洽方法通过思维提示链（chain of thought prompting）从语言模型中采样一组不同的推理路径，然后返回其中最自洽的答案。

不需要额外的人工注释、训练、辅助模型或微调，可直接用于大规模预训练模型。

思维提示链（chain of thought prompting）：提示语言模型生成一系列短句，这些短句模仿一个人在解决推理任务时可能采用的推理过程。

![img](https://ask.qcloudimg.com/http-save/1754229/5b6b2d720744b8b7f0be71d8ce73747f.png?imageView2/2/w/1620)

该方法在一系列算术和常识推理基准上评估自洽性，可以稳健地提高各种语言模型的准确性，而无需额外的训练或辅助模型。

当假设推理过程正确，即使它们是多样化的，在最终答案中往往比不正确的推理过程具有更高的一致性。

自洽（self-consistency）方法具体步骤如下：

> - 首先，使用一组手动编写的思维链示例对语言模型进行提示；
> - 接着，从语言模型的解码器中采样一组候选输出，生成一组不同的候选推理路径；
> - 最后，通过在生成的答案中选择最自洽的答案来集成结果。