# 微调bert用于文本分类

提供一种通用的BERT微调解决方法，最大化利用BERT增强文本分类任务性能。

1. 进一步预训练BERT任务内训练数据或领域内数据；
2. 若有多个相关任务可用，可选用多任务学习微调BERT
3. 为目标任务采用微调策略来微调BERT

三种方法：

1. BERT自身的微调策略，包括文本处理、学习率、不同层选择等
2. 进一步预训练；
3. 多任务微调



微调策略——处理长文本

1. 截断法：head-only / tail-only / head+tail
2. 层级法：将输入文本分成k=L/512个片段，通过BERT获得文本表示

微调策略——不同层的特征

微调策略——灾难性遗忘

微调策略——逐层降低学习率

（为BERT的不同层设置不同的学习率和衰减因子——较低的BERT层赋予较低的学习率能够有效地微调BERT）



进一步预训练有助于提高目标任务的BERT的性能。

