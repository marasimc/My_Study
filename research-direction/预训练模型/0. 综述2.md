# 预训练语言模型（PLMs）

> 内容来自：
>
>  https://d223302.github.io/AACL2022-Pretrain-Language-Model-Tutorial/
>
> https://d223302.github.io/AACL2022-Pretrain-Language-Model-Tutorial/lecture_material/AACL_2022_tutorial_PLMs.pdf

预训练语言模型(PLMs)是在大规模语料库上以自监督方式进行预训练的语言模型。在过去的几年中，这些PLM从根本上改变了自然语言处理社区。传统的自监督预训练任务主要涉及**恢复损坏的输入句子，或自回归语言建模**。在对这些PLM进行预训练后，可以对下游任务进行微调。按照惯例，这些微调包括`在PLM之上添加一个线性层，并在下游任务上训练整个模型；或将下游任务表述为句子补全任务，并以seq2seq的方式微调下游任务`。在下游任务上对PLM进行微调通常会带来非凡的性能提升，这就是plm如此受欢迎的原因。

**在本教程中，从两个角度提供广泛而全面的介绍:为什么这些PLM有效，以及如何在NLP任务中使用它们。**

> - 第一部分对PLM进行了一些有见地的分析，部分解释了PLM出色的下游性能。其中一些结果帮助研究人员设计更好的预训练和微调方法
> - 第二部分首先关注如何将对比学习应用于PLM，以改进由PLM提取的表示，然后说明如何在不同情况下将这些PLM应用于下游任务。这些情况包括在数据稀缺的情况下对PLM进行微调，以及使用具有参数效率的PLM。

## Part 1 Introduction

PLM + fine tune

## Part 2 Why do PLMs work

### 2.1 Contextualized word respresentation

<img src="assets/0. 综述2/image-20221219165351146.png" alt="image-20221219165351146" style="zoom:30%;" />

词向量表示方法，比如Word2Vec/Glove，BERT可以被视为一种先进的词向量表示方法，即上下文词向量表示(contextualized word respresentation)，不仅仅包括：相似的token有相似的embedding表示（这在word2vec/Glove中已经实现了），还考虑了token的上下文信息，因此相同的词可能有不同的词向量表示。

### 2.2 BERTology - What does each layer learn?

<img src="assets/0. 综述2/image-20221219205027334.png" alt="image-20221219205027334" style="zoom:30%;" />



### 2.3 BERT Embryology - What BERT learned during training?

### 2.4 When do you need billions of words of pretraining data

<img src="assets/0. 综述2/image-20221219205956245.png" alt="image-20221219205956245" style="zoom:60%;" />

### 2.5 cross-discipline capability（跨学科能力）

### 2.6 Pre-training on Artificial Data



## Part 3 How to Use PLMs: Contrastive learning for Pre-trained Language Models

**Why Contrastive?（为什么需要对比学习）**

想要在以下场景对词有一个比较好的表示：

> 1. 相似的输入有相似的表示（positive pairs）
> 2. 不相似的输入有不相似的表示（negative pairs）

<img src="assets/0. 综述2/image-20221219214953483.png" alt="image-20221219214953483" style="zoom:80%;" />

<img src="assets/0. 综述2/image-20221220174501232.png" alt="image-20221220174501232" style="zoom:80%;" />

### 3.1 Why we need sentence-level representation?

> - Provide as a backbone that can be useful on a variety of downstream sentence-level tasks（提供可用于各种下游句子级任务的主干）
> - Good generalization ability on tasks without much training data e.g. even linear probing can achieve good performance（良好的泛化能力，不需要大量的训练数据。即使是线性探测也能取得良好的性能）
> - Efficient sentence-level clustering or semantic search by innerproducts（基于内部产品的高效句子级聚类或语义搜索）
> - Measure similarities among sentence pairs（句子对间的度量相似性）
> - Unsupervised methods are more desirable in order to be applied to languages beyond English（非监督的方法是更可取的，以便应用英语以外的语言）

### 3.2 Pre-BERT methods

> <img src="assets/0. 综述2/image-20221220174644078.png" />
>
> <img src="assets/0. 综述2/image-20221220174658830.png"/>

### 3.3 How to obtain sentence-level representations from BERTs

> - 不能简单地从token-level的表示中获得。
>
> - BERT表示空间中的各向异性问题(anisotropy problem)：
>
>   <img src="assets/0. 综述2/image-20221220175955123.png" alt="image-20221220175955123" style="zoom:50%;" />
>
>   - 表示退化（representation degeneration）：学习的嵌入在向量空间中占据一个狭窄的圆锥；
>   - 限制向量空间的表现力
>
> - BERT flow: <img src="assets/0. 综述2/image-20221220180217886.png" alt="image-20221220180217886" style="zoom:80%;" />
>
> - BERT-whitening

### 3.4 Cotrastive learning method

> - Designed positives —— DeCLUTR、ConSERT
> - Generating Positives
> - Bootstrapping Methods —— BYOL
> - Dropout Augmenttions —— SimCSE (Unsupervised)、Supervised SimCSE、mSimCSE
> - Equivariant Contrastive Learning
> - Prompting
> - Ranking-based Methods —— RankEncoder

### 3.5 conlusion

- Contrastive learning should have more potential in NLP for using pre-trained language models in representation learning!


## Part 4 How to Use PLMs: Parameter-efficient fine-tuning

> - Problem: PLMs are gigantic (in terms of numbers of parameters, model size, and the storage needed to store the model)
>
> - Solution: Reduce the number of parameters by parameter-efficient fine-tuning
>
>   <img src="assets/0. 综述2/image-20221220230602165.png" alt="image-20221220230602165" style="zoom:20%;" />       <img src="assets/0. 综述2/image-20221220230533523.png" alt="image-20221220230533523" style="zoom:20%;" />

一个标准的fine-tuning实际执行的操作？-> 更改PLM的隐藏层表示以使得它能够在下游任务更好地表现。<img src="assets/0. 综述2/image-20221220231213504.png" alt="image-20221220231213504" style="zoom:80%;" />

### 4.1 Adapter

<img src="assets/0. 综述2/image-20221220231552781.png" alt="image-20221220231552781" style="zoom:67%;" />

- Adapter：一个被嵌入transformer的小的可训练子模块

  <img src="assets/0. 综述2/image-20221220231736187.png"/>

  <img src = "./assets/0. 综述2/image-20221220232120414.png" />

- 在fine-tuning期间，仅更新adapters与classifier head的参数。

- 通过采用adapter结构，所有下游任务共享PLM参数，每层的adapters以及classifier heads则是特定任务的模块。

### 4.2 LoRA

<img src="assets/0. 综述2/image-20221220232848594.png"/>

- LoRA: Low-Rank Adaptation of Large Language Models

  - 平行地插入transformer的feed-forward层，也可以插入multi-head attention层

  <img src="assets/0. 综述2/image-20221220233010445.png"/>

  

  - 考虑LoRA平行地插入feed-forward层的情况：<img src="assets/0. 综述2/image-20221220233613212.png"/>

  <img src="assets/0. 综述2/image-20221220233827879.png" alt="image-20221220233827879" style="zoom:40%;" />

- 通过采用LoRA结构，所有下游任务共享PLM参数，每层的LoRA以及classifier heads则是特定任务的模块

### 4.3 Prefix tuning

<img src="assets/0. 综述2/image-20221220234333352.png" />

- Prefix Tuning: 在每层之前插入可训练前缀

  <img src="assets/0. 综述2/image-20221220234512542.png"/>

  - 标准的self-attention结构：<img src="assets/0. 综述2/image-20221220235041607.png"/>

  - 加上prefix后的self-attention结构：

    <img src="assets/0. 综述2/image-20221220235208363.png">

  - 

  

- Only the prefix (key and value) are updated during finetuning

### 4.4 (Soft) Prompt tuning



## Part 5 How to Use PLMs: Using PLMs with different amounts of data



## Conclusion and Future work