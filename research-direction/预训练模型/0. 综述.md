# 预训练模型(PTM)综述

***reference：https://www.51cto.com/article/681137.html         http://keg.cs.tsinghua.edu.cn/jietang/publications/AIOPEN21-Han-et-al-Pre-Trained%20Models-%20Past,%20Present%20and%20Future.pdf***

由于复杂的预训练目标和巨大的模型参数，大规模 PTM 可以有效地从大量标记和未标记的数据中获取知识。通过将知识存储到巨大的参数中并对特定任务进行微调，巨大参数中隐式编码的丰富知识可以使各种下游任务受益。

目前，PTM正在向四个重要方向发展：**设计有效的架构、利用丰富的上下文、提高计算效率以及进行解释和理论分析**。

## 迁移学习和有监督预训练

早期预训练的研究主要涉及迁移学习。迁移学习的研究很大程度上是因为人们可以依靠以前学到的知识来解决新问题，甚至取得更好的结果。更准确的说，**迁移学习旨在从多个源任务中获取重要知识，然后将这些知识应用到目标任务中**。

在迁移学习中，源任务和目标任务可能具有完全不同的数据域和任务设置，但处理这些任务所需的知识是一致的。一般来说，**在迁移学习中有两种预训练方法被广泛探索：*特征迁移和参数迁移***。

在一定程度上，特征迁移和参数迁移奠定了 PTM 的基础。词嵌入是在特征迁移框架下建立起来的，被广泛应用于 NLP 任务的输入。

## **自监督学习和自监督预训练**

迁移学习可以分为四个子设置：**归纳（inductive）迁移学习、transductive 迁移学习、自我（self-taught）学习和无监督迁移学习**。

<img src="./assets/0. 综述/image-20221217153204979.png">

