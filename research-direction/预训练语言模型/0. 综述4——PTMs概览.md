# PTMs概览

> reference: https://zhuanlan.zhihu.com/p/353054197

## 1. 预训练任务分类

- 有监督学习（基于input-output pairs）
- 无监督学习（基于unlabeled data）：聚类/密度估计/隐式表示……
- 自监督学习：例如BERT中的masked language model任务

给定一个文本序列 x~1:T~=[x~1~,x~2~,...,x~T~] ，其联合概率分布可以分解为：p(x~1:T~)=∏~t=1~^T^p(x~t~|x~0:t−1~) ,

其中 x~0~ 为序列的起始token。

对每个token，基于神经编码器 fenc 和一个预测层 gLM （例如softmax函数），得到：p(x~t~|x~0:t−1~)=gLM(fenc(x~0:t−1~)) ，可以使用例如最大似然估计(MLE)来训练该模型。

这里是从左向右的处理序列，即当前词的条件概率只和其左边的词序列相关，可以进一步扩展为双向相关。

### 1.1 MLM(Masked language model)：掩码语言模型

#### 1.1.1 原始MLM —— 只使用Transformer的Encoder部分

MLM最早是Taylor等人提出的，受的是”完形填空“的启发，之后**BERT**将其发扬光大了。

为了可以双向训练BERT，Devlin等人提出在Transformer的Encoder的基础上，把输入的句子中的若干词用[MASK]遮盖起来，然后通过其他可见的词来预测被掩盖的词。这里有个问题在于，在实际下游fine-tuning的时候，并没有[MASK]这个token出现在下游数据集中。为了缓解这个问题，BERT中提出：在80%的概率下，使用[MASK]这个token；然后在10%的概率下，使用任意token；以及在剩下的10%的概率下，使用原来的token。（缓兵之计，并不是完美的）。

MLM常被当成分类问题（预测每个被遮掩位置的具体的token的种类，例如词表是30000的时候，则是类似multi-class classification并且|y|=30000

#### 1.1.2 seq2seq MLM

除了仅使用Transformer的Encoder部分构造的MLM之外，还有另一种“自回归生成式模型”的方法，即把masked sequence扔给Encoder，然后再用一个decoder去逐个预测masked token，例如：MASS, T5等。

#### 1.1.3 Enhanced MLM / E-MLM

- 例如**RoBERTa**，使用“**dynamic mask**”来改进BERT，即：BERT是在数据处理阶段将一个sequence中的词确定好是否替换为[mask]，之后再也不修改masked token；而RoBERTa则是在每次构建mini-batch的时候动态地进行一次sampling，确定被mask的词，从而对于一个句子，每次被使用来训练的时候，其已知次（没有被mask的词）和未知词（被mask的词）都是动态的、临时决定的。

- 微软的**UniLM**则对MLM进行扩展：单向（针对例如长文本生成这样的任务）、双向（GLUE基准任务，抽取式阅读理解QA）、以及seq2seq（自动摘要、问题生成、生成式QA）

  <img src="https://pic2.zhimg.com/80/v2-8af2c2a8b12677ed54d38a3869dc7a79_720w.webp" alt="img" style="zoom:0%;" />

  - 单向MLM：为了预测序列" x~1~ x~2~ [MASK] x~4~ "中的[mask]，我们只可以使用 x~1~,x~2~ 
  - 双向MLM：与BERT一样，即 x~1~,x~2~,x~4~ 都可以用来预测[mask]
  - 序列到序列MLM：
  - 设有两个segments(文本序列） S~1~=t~1~ t~2~, S~2~=t~3~ t~4~ t~5~ ，目标是从 S~1~ 生成 S~2~ ，那么这里的限制条件是：在 S~1~ 中是双向可见的，而在 S~2~ 中是从左到右单向可见的。模型的输入是： [SOS] t~1~ t~2~ [EOS] t~3~ t~4~ t~5~ [EOS] ，那么：
    - t1 可以看到 [SOS] t~1~ t~2~ [EOS] ； t2 可以看到 [SOS] t~1~ t~2~[EOS] ；
    - t4 只能看到 [SOS] t~1~ t~2~[EOS] t~3~ t~4~ ，其右边的tokens（ t~5~ [EOS] ）就看不到了。

#### 1.1.4 XLMs - Facebook AI

XLMs即cross-lingual language models，在一个双语序列上执行MLM。具体如下图所示：

<img src="https://pic4.zhimg.com/80/v2-8053cbb142311bcdd11f8d4b3637336b_720w.webp" alt="img" style="zoom:150%;" />

TLMs (translation LMs)把一个parallel sentence 串接起来，扔给BERT。类似于输入的序列为“[/s] 我 爱 你 [/s] I love you [/s]”

#### 1.1.5 SpanBERT (2019, 华盛顿大学，普林斯顿，艾伦人工智能学院，以及facebook AI的学者)

SpanBERT对BERT的改进包括：

- **Random Contiguous Words Masking**：对连续的多个tokens（即一个span内的所有tokens）进行掩码，而不是随机地选择若干（相对独立的，不连续的）tokens进行掩码
- **Span Boundary Objective - SBO**：训练时，用的是span的左右边界，而不是其所包含的tokens

<img src="https://pic1.zhimg.com/80/v2-f1c97917c1188623412d1eb8290fdb18_720w.webp" alt="img" style="zoom:80%;" />

如上图所示，一个包含了四个token的span, "an American football game"整个被mask掉了。

SBO使用边界token（即x~4~,x~9~）去预测masked span的每个token。

span内部的基于传统BERT中的MLM被掩码的词，例如football，这里使用的位置编码，就是“其在span中的位置3，假设an的位置是1，则football的位置就是3，所以上图中用的是 p~3~ ”，而不是football在整个句子中的位置了。

例如，在计算词football的loss的时候，有两个部分，一则是来自mlm=masked language model的loss，即-log P(football | x~7~)，这个x~7~是transformer encoder的最终的在句子的位置7的hidden vector的输出；另外一部分是来自SBO，即-log P(football | x~4~, x~9~, p~3~)，这里的给定的是x~4~, x~9~这两个来自左右边界的token的表示向量，而p~3~表示的是football在被mask掉的phrase，“an american football game”中的相对位置（第三个词）。

该思想也很好的借鉴了抽取式阅读理解的loss function。

#### 1.1.6 StructBERT (2019, 阿里巴巴团队)

StructBERT把结构化的信息引入BERT

（1）词结构目标

<img src="https://pic2.zhimg.com/80/v2-d0d423c7d14ef97bf2577fb8258e5915_720w.webp" alt="img" style="zoom:80%;" />

如上图所示，展示了"word objective"和"MLM objective"的联合训练过程。

- 首先MLM目标通过标准BERT的操作完成：对输入的sequence的15%的tokens进行遮掩，然后基于BERT来预测这些masked tokens。

- 之后，对其中的一个span（包括连续的k个词）进行随机打乱顺序，例如上图的t~2~,t~3~,t~4~被shuffe成了t~3~,t~4~,t~2~，然后"word objective"即是：给定一个被打乱顺序的序列，尝试预测每个被移位的词的原始正确的位置：

  <img src="https://pic3.zhimg.com/80/v2-1363e3d6b1a55eceeab5b81c868d91b2_720w.webp" alt="img" style="zoom:50%;" />

（2）句子结构目标

<img src="https://pic2.zhimg.com/80/v2-32a73c54843b4476453291a3ab296495_720w.webp" alt="img" style="zoom:80%;" />

句子结构目标的核心在于：不单单是预测下一个句子，而且也预测上一个句子。给定一个句子对(S1, S2)，我们预测的是：①S2是S1的下一句；②S2是S1的前一句；③S2是来自其他document的随机抽取的一句（和S1无关）。

因此，在构造训练集合的时候，可以按照三个1/3的方式，分别构造这样的训练集合。（思想和ALBERT的sentence order预测有些类似。）

### 1.2 XLNet —— PLM方法

因为很多下游应用任务中，并没有包括[MASK]的文本序列，所有传统MLM和下游任务之间存在一定的沟壑，为了缓解该问题，==Permuted Language Modeling(PLM)==被提出。

PLM任务是，首先对一个输入序列构造所有可能的全排列，然后选择其中一个全排列，此后，该全排列中的若干词被选择为target，模型的训练是`基于剩余的tokens以及targets的原始的正确的位置信息`去预测这些targets。

<img src="https://pic3.zhimg.com/80/v2-a755ca8fb4b7b4f76184de5706427e96_720w.webp" alt="img" style="zoom:90%;" />

当给定了一个全排列的时候，如何根据此全排列制定的”可见上下文“来预测同一个词 x~3~。例如左上角的时候，当给定的全排列是3->2->4->1的时候，则2,4,1对于预测 x~3~ 都不可见，所以只能是有 mem^(0)^ 作为输入。类似的，右下角，当给定的全排列是4->3->1->2的时候，则在预测 x~3~ 的时候，只有 x~4~ 可见。

给定一个文本序列x = [x~1~,x~2~,...,x~T~]：

- 传统自回归语言模型：最大化“前向自回归分解”下的似然度：<img src="https://pic1.zhimg.com/80/v2-c4bd7d82f7dfca89135143b2a3c05994_720w.webp" alt="img" style="zoom:40%;" />，其中，h~θ~(x~1:t-1~)为神经模型产出的上下文表示，e(x)是token x的词嵌入表示向量。

- BERT中的MLM：训练目标函数为：<img src="https://pic4.zhimg.com/80/v2-4279538956ca756ad45675d8843c603b_720w.webp" alt="img" style="zoom:40%;" />，其中，x^^^ 为掩码后得到的新序列，x^—^为被掩码的tokens组成的（不连续的）序列，m~t~=1表示x~t~被掩码了，H~θ~代表了Transformer，负责把长度为T的序列映射到一个向量列表。

  > 存在的问题：
  >
  > 1. 独立性假设，即认为被mask掉的tokens是彼此相互独立的，一个token的预测和另外一个token的预测独立；这显然是有所偏颇的；
  > 2. 输入噪声，[MASK]不会出现在NLP下游任务的文本中，那么作为输入的 x¯ 这里就和BERT的实际应用”脱节“了；
  >
  > 优点：上下文依赖，MLM参考了双向上下文信息，但是自回归LM只有单向。

XLNet的改进：

1. 保留了自回归LM的优点（不用[mask]）；
2. 同时看左右两个方向上的上下文。

优化目标：<img src="https://pic3.zhimg.com/80/v2-15e6beed844e221bb7a971f66a5ee2c2_720w.webp" alt="img" style="zoom:33%;" /> 其中，Z~T~表示长度为T的序列的所有的全排列组成的集合；z∈Z~T~是其中的一个全排列；x~z~~t~ 代表在全排列z下的第t个位置的token；z<t表示该全排列下的前t-1个token组成的序列。因为参数 θ 在训练中被所有的全排列所共享使用更新，数学期望上， x~t~ 可以看到其左右的所有的上下文tokens。因此，双向语境成为可能。另外，因为拆解过程，是遵循自回归LM的，所以其也避免了（1）独立性假设，（2）输入噪声即train-finetune之间的沟壑。

### 1.3 DAE：去噪自编码器（denoising autoencoder - DAE）

DAE：把局部坍塌（shuffled)的输入序列，恢复成原来没有被打乱顺序的序列

有若干坍塌输入序列的方法：

1. Token Masking, 随机选择若干tokens，并替换为[MASK]；
2. Token Deletion: 随机删除若干词，然后模型预测”哪个位置“的词，缺失了；
3. Text Infilling: 类似于structBERT，若干文本span被选出，并被替换成一个[MASK]，每个span的长度来自 λ=3 的泊松分布的采样；模型需要预测一个span中多少词是缺失的；
4. Sentence Permutation: 打乱一个文档中的句子的顺序；
5. Document Rotation: 根据均匀分布，从一个文档中选取一个token x，然后文档就是形如A x B这样，然后选择文档到x B A，基于x B A，去预测文档原本的开头的词的位置（A序列中的第一个词在原始文档中的位置）。

### 1.4 对比学习（constrastive learning - CTL）

出发点：一些观察到的词对(x, y)，会比随机采样得到的词对，具有更大的语义相似度。从而学习一个打分函数 s(x,y) ，使用的目标函数是最小化如下”负对数值”：<img src="https://pic2.zhimg.com/80/v2-df3f18e36d0dc0220350662a1606f629_720w.webp" alt="img" style="zoom:50%;" /> 其中， (x,y+) 是正例（相似对）； y− 被看成和x不相似（负例）。

s(x,y) 可以通过如下两种方式学习：

1. s(x,y) = f~enc~^T^(x)f~enc~(y) ：先分别经过神经编码器，得到两个向量表示，然后做内积，得到标量相似度；
2. s(x,y) = f~enc~(x⊕y) ：先把x和y的对应向量表示element-wise相加，然后走神经编码器，再经过一个linear layer得到scalar标量相似度。

相比于LM，CTL一般具有更小的计算量，也是LM一种可行的替代。

例如Collobert等人提出了pairwise ranking，来区分真实的和虚假的短语。其中虚假短语可以通过替换掉短语中的中心词为其他任意词，而得到。

NCE-Noise Contrastive Estimation - 噪声对比估计 - 是使用一个二元分类器来区分真实的和捏造的samples。

### 1.5 对比学习（CLT）变体

#### 1.5.1 Deep InfoMax (DIM)

​		DIM最初兴起于Images，其通过最大化“图片表示”和“图片局部区域的表示”的互信息(mutual information)来提高（模型对图片的）表示能力；Kong等人把DIM应用到了NLP领域：假设在一个序列 x 的最左端加上[CLS]，或者设其左边第一个token就是[CLS]，那么 x 的表示就是被设定为[CLS] 的向量，该向量通过神经编码器 fenc(x) 得到。其训练目标是让：f~enc~(x~i:j~)^T^f~enc~(x^ ~i:j~)^T^ > f~enc~(x~ ~i:j~)^T^f~enc~(x^ ~i:j~)^T^ . 其中， x~i:j~ 代表span为i到j的n-gram； x^ ~i:j~ 代表位置i到j被遮掩的序列； x~ ~i:j~ 代表的是一个从语料库中随机采样出来的一个n-gram phrase。

#### 1.5.2 Replaced Token Detection (**RTD**)

给定上下文，判断一个词是否被替换掉了。**ELECTRA** (ICLR 2020文章，来自斯坦福大学和Google Brain；[https://arxiv.org/pdf/2003.10555.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2003.10555.pdf)）使用一个生成器来替换一个序列中的若干tokens。参照GAN的思想，一个生成器G和一个判别器D通过如下两阶段过程训练：

其一，基于MLM任务，对G进行训练（n1步）；

其二，使用G的权重，对D进行初始化；

其三，基于一个判别式任务对D进行训练(n2步）：目标是判断输入的token是否被G替换掉了。

其四，预训练之后，G被丢弃，我们只使用D对下游任务进行fine-tune。

### 1.6 NSP / SOP

**NSP (Next Sentence Prediction) - 下句预测；SOP (Sentence Order Prediction) - 句子顺序预测**

首先基于标点符号划分句子，然后判断两个句子是否有前后文的关系。在构造训练数据的时候，50%的情况下，前后两个句子被选取作为正例；另外50%的情况下，一个句子被选定，然后另外一个句子来自整个语料库的采样。

NSP可以潜在对如下一些任务有好的影响：问答QA，自然语言推论（类似判断两个句子描述是否存在逻辑上的“蕴含”关系）。

主要问题：来自50%的下句来自对全体语料库的采样。这样，单纯的different topics会把原本的“上下句子位置关系“给冲洗掉。例如ALBERT（ICLR2020论文，来自Google Research和芝加哥丰田技术中心）就提案，去预测句子的前后关系，而不是随机从大的语料库中随机采样一个句子为负例。structBERT里面的前文，后文，无关系句子的判定，也是类似的思想。其他类似的还有，BERTje (荷兰语的BERT)。

### 1.7 其他预训练任务

- 引入事实性知识；
- 提高多语言任务；
- 多模态（例如文字，语音，图片，视频等）。

### 1.8 PTMs的分类标准

- 表示类型：上下文无关和上下文相关；
- 架构：LSTM （例如ELMo), Transformer的encoder (例如BERT)，Transformer的Decoder （例如GPT)，以及同时使用Transformer的Encoder和Decoder （例如, Seq2Seq MLM)。
- 预训练任务类型，如传统自回归LM，Masked-LM，Permuted-LM, DAE, CTL, 等。
- 扩展：例如，知识强化的PTMs，多语言或者单语言PTMs，多模态PTMs，特定领域PTMs(例如Healthcare, Finance等），以及压缩PTMs。

<img src="https://pic2.zhimg.com/80/v2-5ffcca19bed501f326c2407c1724d8b5_720w.webp" alt="img" style="zoom:80%;" />

<img src="https://pic1.zhimg.com/80/v2-68d27032299b10c8f1e919a5362df200_720w.webp" alt="img" style="zoom:80%;" />



## 2. 预训练任务的损失函数

- LM = 自回归语言模型, auto-regressive language models
- MLM = 掩码语言模型，masked language models
- Seq2Seq MLM=序列到序列的遮掩语言模型，Sequence-to-Sequence Masked Language Model;
- PLM = Permutated language model, 重新排列语言模型
- DAE = 去噪自编码器，denoising auto-encoder
- DIM = Deep InfoMax
- NSP/SOP=next sentence prediction, sentence ordering prediction, 下一个句子预测，句子顺序预测；
- RTD = replaced token detection, 被替换掉的token的检测

<img src="https://pic4.zhimg.com/80/v2-cd1f18e2a96a4c5840bdca2cd433f13f_720w.webp" alt="img" style="zoom:150%;" />

## 3. PTMs分析

PTMs所捕捉到的信息有：

- 上下文无关词嵌入
- 上下文相关的嵌入表示
  - 语言学知识：BERT在一些语法任务，例如POS词性标注，成分标注等任务上表现良好；但是在语义分析，以及细粒度的语法任务上表现欠佳。此外，BERT中被确认囊括了subject-verb agreement，例如He drinks, I eat, They drink等；以及语义角色信息。还有，BERT也用于抽取语法依存树(dependency parsers)或者成分树(PCFG tree structures）。
  - 世界知识，例如Dante was born in [MASK]这样的，会对出生地进行预测，从而得到世界知识（谁，在哪里，什么时间，做了什么，等等）。例如百度的ERNIE会对named entitied进行整体mask，从而更好的对世界commen sense知识进行建模，有兴趣的同学可以参考我的一篇对其的解读的文章：

## 4. PTMs的扩展

### 4.1 用知识强化的PTMs

​		以往一直关注的是如何利用PTMs来挖掘新的知识；将外部知识注入到PTMs中在近年有若干方向的研究，外部知识可分类为：

- linguistic - 语言学知识；
- semantic - 语义知识；
- commonsense - 常识；
- factual - 事实知识；
- domain-specific knowledge - 特定领域知识。


早期研究侧重于研究联合对知识图谱的嵌入和词嵌入进行表示学习。自从BERT横空出世之后，一些辅助性的预训练任务被设计为把外部知识并入到深度PTMs中。其中比较有代表性的有：

1. LIBERT (linguistically-informed BERT) [https://arxiv.org/pdf/1909.02339v1.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.02339v1.pdf)
2. LA-MLM (Label-Aware MLM) [https://arxiv.org/pdf/1911.02493.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.02493.pdf) [清华大学]
3. SenseBERT, [https://arxiv.org/pdf/1908.05646.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1908.05646.pdf)
4. ERNIE(THU) [https://arxiv.org/pdf/1905.07129.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.07129.pdf) (注意不是百度的那个；这个是清华+华为的）
5. KnowBERT，EMNLP2019论文：[https://www.aclweb.org/anthology/D19-1005.pdf](https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/D19-1005.pdf)
6. KEPLER, [https://arxiv.org/pdf/1911.06136.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.06136.pdf)
7. K-BERT, [K-BERT: Enabling Language Representation with Knowledge Graph](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.07606) (来自北大，腾讯，和北师大）
8. Xiong et al., ICLR 2020论文，Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model：[https://arxiv.org/pdf/1912.09637.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1912.09637.pdf)
9. K-Adapter, [https://arxiv.org/pdf/2002.01808.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.01808.pdf) （在微软实习的复旦实习生的作品，后边作者一堆微软的大佬们）
10. ConceptNet + ATOMIC -> GPT-2 故事生成，[https://arxiv.org/pdf/2001.05139.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2001.05139.pdf)
11. Yang et al., Enhancing pre-trained language representations with rich knowledge for machine reading comprehension. ACL 2019文章：[https://www.aclweb.org/anthology/P19-1226.pdf](https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/P19-1226.pdf)
12. KGLM + LRLM，分别是：Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling，ACL 2019的文章。[https://www.aclweb.org/anthology/P19-1598.pdf](https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/P19-1598.pdf)，以及：[https://arxiv.org/pdf/1908.07690.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1908.07690.pdf)，Latent Relation Language Models。

`概要的介绍参考链接：`https://zhuanlan.zhihu.com/p/353054197
