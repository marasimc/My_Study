## 1. copy mechanism

Copy Mechanism 是 NLP 中一种机制，用于解决文本生成任务中的复制问题。在文本生成任务中，当待生成的内容与原文中已有的内容重复时，传统的生成模型容易出现复制失误的情况。Copy Mechanism 的作用就是对该问题进行修正。

Copy Mechanism 通常是针对 Seq2Seq 模型进行设计的，其主要思想是==在 Decoder 阶段，将输入序列中的一些词汇直接复制到输出序列中，避免使用生成模型产生的错误==。具体来说，==Copy Mechanism 通过计算某个词汇在输入序列中的出现概率，计算出该词汇对应的 copy 概率，并将其加入到 Seq2Seq 模型生成结果的概率计算中。==

Copy Mechanism 的实现方式有多种，其中一种比较常见的方法是使用注意力机制（Attention Mechanism），结合 RNN 网络，将 Encoder 输出的源语言表示作为注意力机制的输入，计算出每个源语言位置的注意力分配权重，然后通过加权求和的方式得到一个源语言向量表示，以此实现文本的复制。这种做法可以使模型自适应不同的输入长度和数量。

Copy Mechanism 在 NLP 中得到了广泛的应用，例如在机器翻译、摘要生成等任务中，都可以通过引入该技术提升模型的性能。

## 2. 指针网络（pointernet）



## 3. 二元门控（binary gate）



## 4. torch.nn

### 4.1 torch.nn.DataParallel

用于在多个 GPU 上并行计算神经网络的前向传递和反向传播过程。它将一个大型的模型拆分成多个小型子模型，在每个 GPU 上独立运算，并将结果最终汇总到主 GPU 上，从而加快训练速度。

```
torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)

module: 待并行处理的模型
device_ids: gpu编号
output_device: 表示输出结果所在的gpu编号（默认使用与输入数据相同的设备）
dim: 表示沿着哪个维度拆分输入数据，默认为0，即沿着批次维度拆分
```

