# TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation

> 论文地址：
>
> 代码地址：

## Abstract

​		本论文提出了TRANX，这是一种基于语义转换的神经语义解析器，将自然语言（NL）映射为形式意义表示（formal meaning representation, MRs）。TRANX对目标MR使用一个基于抽象语法描述语言的转换系统，具有两点主要的优势：（1）具有较高的准确性，可以利用目标MR的语法信息来约束输出空间和建模信息流；（2）具有较高的泛化能力，只需根据目标MR中允许的结构重新构造一个抽象语法描述，就可以很容易地应用于新的MR类型。在四种不同的语义解析和代码生成任务上的实验表明本系统具有较强的泛化能力、可扩展能力、有效性，与现有的神经语义解析器相比也取得了更优的结果。

## 1. Introduction

​		语义解析是一个将自然语言转换为形式意义表示（MRs）的任务，目标MRs可以根据各种形式来定义，这包括旨在捕捉任意句子含义的语言动机（linguistically-motivated）语义表征，如λ演算和抽象意义表征。或者，对于更多的用于语义解析的任务驱动方法，通常是用意义表示来表示可执行程序（如SQL查询、机器人命令、智能手机指令、甚至是通用编程语言如Python/java）。

**背景：** 由于MRs各种各样的形式，语义解析器（特别是基于神经网络的~）的设计关注于一小部分子任务，以保证生成的MRs的语法格式良好。最近研究尝试使用通用语法模型进行神经语义解析：①一种神经seq-to-seq模型，在先验提供给模型的特定于任务的上下文无关文法的指导下，使用一系列树构造操作生成树结构的MRs；②抽象语法网络（abstract syntax networks, ASNs），其中特定领域的MRs由抽象语法树（ASTs）表示，这些树是在抽象语法描述语言（abstract syntax description language, ASDL）框架下指定的；ASN采用模块化结构，使用专门设计的神经网络为ASDL语法中的每个结构生成AST.

受现有研究的启发，本文提出了TRANX，一个用于语义解析和代码生成的基于转换的抽象语法解析器，其设计遵循以下原则：

- **Generalization ability：** TRANX应用ASTs作为通用的中间意义表示，特定任务相关文法作为额外的指导解析过程的知识提供给系统，从而实现了语义解析过程和特定文法的解耦（decoupling）
- **Extensibility：** TRANX使用一个简单的转换系统（transition system）去讲NL解析为树结构的ASTs，转换系统被设计为易于扩展的，需要最少的工程来适应需要处理额外领域特定信息的任务；
- **Effextiveness：**  在四种语义解析任务（ATIS, GEO）和代码生成任务（DJANGO, WIKISQL）上进行测试，证明了TRANX具有在不同领域的泛化能力，同时具有强大的性能，在四个数据集（GEO, ATIS, DJANGO）中的三个上优于现有的基于神经网络的方法。

## 2. Methodology

给定一个NL语句，TRANX将该语句解析为形式意义表示（通常为λ演算逻辑形式、特定领域或通用编程语言），以Python代码生成为例，Fig. 1描述了TRANX的工作流：

<img src="assets/4. TRANX： A Transition-based Neural Abstract Syntax Parser for Semantic/image-20230109170702774.png" alt="image-20230109170702774" style="zoom:150%;" />

TRANX的核心是一个转换系统（transition system）。

- 给定一个输入NL语句x，TRANX利用转换系统使用一系列树构造操作将语句x映射为AST z，使用ASTs作为中间语义表示，对MRs的特定领域结构进行抽象。解析的过程由ASDL形式规定的用户定义的特定领域的文法指导。
- 给定生成的AST z，解析器调用用户定义的函数AST_to_MR(.)，将中间AST转换为特定领域表示y。

TRANX使用一个由神经网络参数化的概率模型p(z|x)对每个假设AST进行评分。

### 2.1 Modeling ASTs using ASDL Grammer

TRANX使用ASTs作为通用的用于MRs的中间语义表示，ASTs通常用于表示编程语言，也能够被用于表示其他树结构的MRs (eg. λ-calculus)。ASDL框架是一个定义ASTs的文法形式。

==ASDL文法有两个基本的结构：types（类型）和constructors（构造函数）==：

- 复合类型(composite types)：由该类型下的构造函数集定义，例如，FIg.1中`stmt`和`expr`复合类型分别表示Python语句和表达式，分别由一系列的构造函数定义；
- 构造函数：使用fields指定特定类型的语言构造。例如，复合类型`expr`下的`Call`构造函数表示函数调用表达式，它有三个字段(fields)：`func, args, keywords`. 
- 一个构造函数中的每个field也是强类型的，它指定了该字段可以保存的值的类型；一个带有复合类型的field可以由相同类型的构造函数实例化，例如，func字段可以保存expr类型的构造函数；还有一些带有基本类型的字段，用于存储值。例如，Name构造函数的id字段有一个基本类型标识符（identifier），用于存储标识符名称；Str(string)构造函数中的s字段保存字符串字面值；最后，每个字段都有一个基数（single, optional ? and sequential *），表示该字段所持有的值的数量。

一个AST由多个构造函数组成，树上的每个节点对应于构造函数中的一个类型化字段（根节点除外，根节点表示根构造函数）。根据字段的基数，一个节点可以持有一个或多个构造函数作为其值。例如，Fig. 1的ASDL Grammar中带有单个基数的func字段用一个Name构造函数实例化，而具有序列基数的args字段有多个子构造函数。

### 2.2 Transition System

开发了一个Transition system，将生成AST的过程分解为一系列构造树的操作（tree-constructing actions）。

<img src="assets/4. TRANX： A Transition-based Neural Abstract Syntax Parser for Semantic/image-20230110091806917.png" alt="image-20230110091806917" style="zoom:150%;" />

如Fig. 2所示，右边列出了一系列用于构建样例AST的树构造操作：在高层，生成过程从单个根节点的初始推导AST开始，按照自顶向下、从左到右的顺序遍历AST；在每个时间步，以下三种操作类型之一被用于扩展推导的开放边界域（opening frontier field n~ft~）：

- **APPLYCONSTR**[c] 将一个构造函数c应用到与c类型相同的开放复合边界域，使用c中的字段填充起始节点，如果边界域具有顺序的基数，动作将构造函数附加到字段持有的构造函数列表中；
- **REDUCE** 标记具有可选（?）或多个（*）基数的字段子值的生成完成；
- **GENTOKEN**[V] 动作用一个token v填充一个（空的）原始边界字段，例如，图2中的字段f~7~具有类型标识符（type identifier），并使用单个GENTOKEN操作实例化；对于string类型字段（比如f~8~），它的值可以由多个tokens组成，可以使用GENTOKEN操作序列来填充它，带有一个特殊的GENTOKEN[</f>]操作用于终止token值生成的动作。

当推导过程没有边界字段时生成就会完成，TRANX之后调**用函数AST_to_MR(.)将生成的中间AST z转换为目标特定领域的MR y**，TRANX提供了多种辅助函数以简化写转换函数的过程；同时提供了几个内置的转换函数，以处理语义解析和代码生成中常用的MRs。

### 2.3 Computing Action Probabilities p(z|x)

给定转换系统，一个AST z的概率被分解为用于生成z的一系列操作的概率：
$$
p(z|x) = \prod _t  p(a_t|a<t, x)
$$
使用一个具有增强训练连接的encoder-decoder框架来参数化p(z|x)，以反映ASTs的拓扑结构。

- **Encoder** encoder是一个标准的双向LSTM网络，将带有n个tokens的输入语句x，{x~i~}~i=1~^n^ 编码为向量表示{h}~i=1~^n^ 

- **Decoder** decoder同样是一个LSTM网络，其每个时间隐藏状态s~t~ 为：
  $$
  s_t = f_{LSTM}([a_{t−1} : \widetilde{s}_{t−1} : p_t], s_{t−1}),
  $$
  其中，f~LSTM~ 是LSTM转化函数，[:]表示向量连接，a~t-1~是上一时间步的嵌入。为每个操作维护一个嵌入向量，\widetilde{s}_{t}是注意力向量，定义为：$ \widetilde{s}_t = tanh(W_c[c_t: s_t]) $
  $$
  \widetilde{s}_t = tanh(W_c[c_t: s_t])
  $$
  其中，c~t~是使用注意力机制从输入编码{h~i~}~i=1~^n^ 检索到的上下文向量。



- **Parent Feeding**：p~t~是一个向量，在推导过程中将父边界域n~ft~的信息进行编码，它是两个向量的拼接：边界域n~ft~的embedding，以及APPLYCONSTR操作生成n~ft~构造函数时的解码器状态s~pt~。Parent Feeding反映了树状结构的AST的拓扑结构，并在生成复杂的MRs（如Python代码）时提供了更好的性能。

- **Action Probabilities**：带有emedding a~c~的APPLYCONSTR[c]操作的概率为：$p(a_t = APPLYCONSTR[c]|a_{<t}, x) = softmax(a_c^T W \widetilde{s}t) $   对于GENTOKEN操作，应用一个生成/复制混合方法，允许在x中超出词汇表（oov, out-of-vocabulary）的变量名称和字面量（例如，图1中的"file.csv"）直接复制到派生中。特别地，操作概率被定义为边缘概率：
  $$
  p(a_t = GENTOKEN[v]|a_{<t}, x)
  \\ = p(gen|a_t, x)p(v|gen, a_t, x)+p(copy|a_t, x)p(v|copy, a_t, x)
  $$
  二元概率p(gen|.) 和p(copy|.)由$softmax(W \widetilde{s}_t)$ 给出，从封闭集词汇表p(v|gen, .)生成v的概率定义类似于$p(a_t = APPLYCONSTR[c]|a_{<t}, x) = softmax(a_c^T W \widetilde{s}t) $ 。x的第i个词的复制概率使用一个指针网络定义：
  $$
  p(x_i|copy, a_{<t}, x) = softmax(h_i^TW\widetilde s_t)
  $$

## 3. Experiments

### 3.1 Datasets

在四个语义解析和代码生成任务上进行实验：

#### 3.1.1 Semantic Parsing

在GEO和ATIS数据集上进行评估。两个数据集上的MRs以λ-

#### 3.1.2 Code Generation

在通用（Python， DJANGO）和特定领域（SQL, WIKISQL）代码生成任务上进行评估。

### 3.2 Results

所有的结果是使用不同的随机数种子运行三次得到的平均值。

（1）**Semantic Parsing**

（2）**Code Generation**





