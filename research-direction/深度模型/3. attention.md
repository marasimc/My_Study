# attention

> [一文读懂「Attention is All You Need」| 附代码实现 (qq.com)](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247486960&idx=1&sn=1b4b9d7ec7a9f40fa8a9df6b6f53bbfb&chksm=96e9d270a19e5b668875392da1d1aaa28ffd0af17d44f7ee81c2754c78cc35edf2e35be2c6a1&token=2009012419&lang=zh_CN&scene=21#wechat_redirect)

attention层自身存在的不足：

- Attention 虽然跟 CNN 没有直接联系，但事实上充分借鉴了 CNN 的思想，比如 Multi-Head Attention 就是 Attention 做多次然后拼接，这跟 CNN 中的多个卷积核的思想是一致的；还有论文用到了残差结构，这也源于 CNN 网络。 
- 无法对位置信息进行很好地建模，这是硬伤。尽管可以引入 Position Embedding，但我认为这只是一个缓解方案，并没有根本解决问题。
- 并非所有问题都需要长程的、全局的依赖的，也有很多问题只依赖于局部结构，这时候用纯 Attention 也不大好。